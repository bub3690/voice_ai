{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252f6258",
   "metadata": {},
   "source": [
    "# MSF 모델\n",
    "\n",
    "- mfcc, spectrogram , mel-spectrogram 3채널을 엮어서 학습을 해본다.\n",
    "- 이미지는 300 x 300 으로 고정하고, mfcc 100을 interpolate 한다.\n",
    "- fusion 실시 x\n",
    "- grad-cam 포함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505136e",
   "metadata": {},
   "source": [
    "- http://keunwoochoi.blogspot.com/2016/03/2.html\n",
    "- http://www.rex-ai.info/docs/AI_Example_CNN_speech_recognize\n",
    "- https://www.youtube.com/watch?v=oltGIc4uo5c\n",
    "- https://youdaeng-com.tistory.com/5\n",
    "- https://quokkas.tistory.com/37 : early stopping\n",
    "- https://continuous-development.tistory.com/166 : stratified kfold\n",
    "- https://deep-learning-study.tistory.com/476 fiter 시각화\n",
    "- https://wyatt37.tistory.com/10 : random over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"SVD-voice-disorder\", entity=\"bub3690\")\n",
    "wandb.run.name = 'MSF-organic-baseline'\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # 인공 신경망 모델들 모아놓은 모듈\n",
    "import torch.nn.functional as F #그중 자주 쓰이는것들을 F로\n",
    "from torchvision import transforms, datasets\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "import librosa, librosa.display \n",
    "\n",
    "p = os.path.abspath('../../..') # 상위 폴더를 사용하기 위해서.\n",
    "sys.path.insert(1, p)\n",
    "from pytorchtools.pytorchtools import EarlyStopping # 상위 폴더에 추가된 모듈.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "#DEVICE = torch.device('cpu')\n",
    "print('Using Pytorch version : ',torch.__version__,' Device : ',DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace19be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38dcdc",
   "metadata": {},
   "source": [
    "# 데이터 나누기 - Stratified KFold\n",
    "\n",
    "- pathology : 1194 / healthy : 634 / 총 1828\n",
    "- k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a148977",
   "metadata": {},
   "source": [
    "## 1. test/ train 나누기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9bf92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1. train, test 나누기\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split # train , test 분리에 사용.\n",
    "\n",
    "\n",
    "pathology = glob('D:/project/voice_pathology_ai/voice_data/organics/pathology/phrase/*.wav')\n",
    "healthy = glob('D:/project/voice_pathology_ai/voice_data/organics/healthy/phrase/*.wav')\n",
    "print(\"Pathology : \",len(pathology))\n",
    "print(\"Healthy: \",len(healthy))\n",
    "\n",
    "pathology= [ path.split(\"\\\\\")[-1] for path in pathology] # path 데이터 변환.\n",
    "healthy= [ path.split(\"\\\\\")[-1] for path in healthy] # path 데이터 변환.\n",
    " # path 데이터 변환 #외부데이터로 가져오기위해서, 번호만 남긴다\n",
    "\n",
    "    \n",
    "X = pathology+healthy # path 데이터 합\n",
    "print(\"총 데이터수 : \",len(X))\n",
    "Y = [] # 라벨\n",
    "for idx,x in enumerate(X):\n",
    "    if idx<597:\n",
    "        Y.append(\"pathology\")\n",
    "    else:\n",
    "        Y.append(\"healthy\")\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y, random_state=456)\n",
    "#stratify를 넣어서, test에도 라벨별 잘 분류되게 한다.\n",
    "\n",
    "print(\"---\")\n",
    "print(\"훈련 셋 : \",len(Y),Counter(Y))\n",
    "print(\"테스트 셋 : \",len(Y_test),Counter(Y_test))\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a99443",
   "metadata": {},
   "source": [
    "## 2. stratified k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ceb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. train, test 나누기\n",
    "#stratified kfold\n",
    "import os\n",
    "import random #데이터 shuffle 사용\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "skf_iris = StratifiedKFold(n_splits=5,shuffle=True,random_state=456)\n",
    "cnt_iter = 0\n",
    "\n",
    "X_train_list = [] #데이터 셋 보관\n",
    "Y_train_list = []\n",
    "\n",
    "X_valid_list = []\n",
    "Y_valid_list = []\n",
    "\n",
    "for train_idx, test_idx in skf_iris.split(X,Y):\n",
    "    \n",
    "    #split으로 반환된 인덱스를 이용하여, 학습 검증용 테스트 데이터 추출\n",
    "    cnt_iter += 1\n",
    "    X_train, X_valid = [X[idx] for idx in train_idx.tolist() ], [X[idx] for idx in test_idx.tolist() ]\n",
    "    Y_train, Y_valid = [Y[idx] for idx in train_idx.tolist() ], [Y[idx] for idx in test_idx.tolist() ]\n",
    "    \n",
    "    X_train_list.append(X_train)\n",
    "    X_valid_list.append(X_valid)\n",
    "    \n",
    "    Y_train_list.append(Y_train)\n",
    "    Y_valid_list.append(Y_valid)\n",
    "    \n",
    "    \n",
    "    #학습 및 예측\n",
    "    \n",
    "    label_train = Y_train\n",
    "    label_test = Y_valid\n",
    "    unique_train, train_counts = np.unique(label_train, return_counts = True)\n",
    "    unique_test, test_counts = np.unique(label_test, return_counts = True)\n",
    "    \n",
    "    uniq_cnt_train = dict(zip(unique_train, train_counts))\n",
    "    uniq_cnt_test = dict(zip(unique_test, test_counts))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('교차 검증 : {}'.format(cnt_iter))\n",
    "    print('학습 레이블 데이터 분포 : \\n', uniq_cnt_train)\n",
    "    print('검증 레이블 데이터 분포 : \\n', uniq_cnt_test,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0880b5c",
   "metadata": {},
   "source": [
    "## 3. random over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. random over sampling\n",
    "for i in range(5):\n",
    "    X_temp = np.array(X_train_list[i]).reshape(-1,1)#각 데이터를 다 행으로 넣음. (1194,1)\n",
    "    #Y = np.array(Y)\n",
    "    ros = RandomOverSampler(random_state = 123)\n",
    "    X_res,Y_res = ros.fit_resample(X_temp,Y_train_list[i])\n",
    "    \n",
    "    print(\"\\n fold{} \".format(i))\n",
    "    print('before dataset shape {}'.format(Counter(Y_train_list[i])) )\n",
    "    print('Resampled dataset shape {}'.format(Counter(Y_res)) )   \n",
    "    \n",
    "    #원래대로 돌리기\n",
    "    X_res=X_res.reshape(1, -1)\n",
    "    X_train_list[i]=X_res[0].tolist()\n",
    "    Y_train_list[i]=Y_res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    " \n",
    "#load\n",
    "with open(\"D:/project/voice_pathology_ai/voice_data/organics/phrase_sig_dict.pickle\",\"rb\") as fr:\n",
    "    phrase_dict = pickle.load(fr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a663f0",
   "metadata": {},
   "source": [
    "# 데이터 정의\n",
    "- 추가적으로 데이터의 크기를 맞춰주기 위해 3초로 padding 및 truncate 실시 https://sequencedata.tistory.com/25 FixAudioLength\n",
    "- 논문에서는 400frame으로 설정.(여기서는 300frame)\n",
    "- 전처리 방법 결정.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=16000\n",
    "win_length =  np.int64(sr/40) # \n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "\n",
    "class svd_dataset(Dataset):\n",
    "    def __init__(self,data_path_list,classes,data_num,training,transform=None,augment=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list[data_num]\n",
    "        self.data_num = data_num\n",
    "        self.training = training\n",
    "        self.label = svd_dataset.get_label(self.path_list,training,data_num)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        self.augment=augment\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list,training,data_num):\n",
    "        label_list=[]\n",
    "        \n",
    "        if training:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_train_list[data_num][idx])\n",
    "        else:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_valid_list[data_num][idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다.     \n",
    "    \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc, spectro, mel-spectro를 추출\n",
    "        2. mfcc를 224프레임으로 패딩. 또한 세로축은 224으로 interpolate\n",
    "        3. resnet에 사용되기 위해 3채널로 쌓기.\n",
    "        4. 미정. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig =  phrase_dict[self.path_list[idx]] # 16000hz 실시\n",
    "        \n",
    "        \n",
    "        MFCCs = librosa.feature.mfcc(y=sig, sr=sr,win_length=win_length ,n_fft=n_fft, hop_length=hop_length, n_mfcc=128)\n",
    "        #MFCCs = librosa.util.normalize(MFCCs) # l-infinity norm\n",
    "        #MFCCs=cv2.resize(MFCCs,(MFCCs.shape[1],128),interpolation=cv2.INTER_LINEAR)# interpolate 적용해서 128 사이즈로\n",
    "        \n",
    "        \n",
    "        \n",
    "        stft = librosa.stft(sig, win_length=win_length,n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        mel_feature = librosa.feature.melspectrogram(y=sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        #stft 300 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "\n",
    "        magnitude = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(magnitude)\n",
    "        #log_spectrogram = librosa.util.normalize(log_spectrogram) # l-infinity norm\n",
    "        \n",
    "        \n",
    "        #padding\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        log_spectrogram = pad2d(log_spectrogram, length)\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        MFCCs = pad2d(MFCCs, length) # mfcc 대신 encoder를 가져와서 해보자.\n",
    "        log_spectrogram=log_spectrogram[:128,:]# 224 x 300 으로 사이즈 조절\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            log_spectrogram=self.transform(log_spectrogram).type(torch.float32)# 타입 변화\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 타입 변화\n",
    "            MFCCs=self.transform(MFCCs).type(torch.float32)# 타입 변화            \n",
    "            \n",
    "            MSF = torch.stack([log_spectrogram,mel_feature,MFCCs])# 3채널로 복사.\n",
    "            #augmentation\n",
    "            if self.training and self.augment:\n",
    "                MSF = MSF.squeeze(dim=1)\n",
    "                MSF=self.augment(MSF)\n",
    "            else:\n",
    "                MSF = MSF.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            ##################안쓰는 곳\n",
    "            log_spectrogram = torch.from_numpy(log_spectrogram).type(torch.float32)\n",
    "            log_spectrogram=log_spectrogram.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return MSF, self.classes.index(self.label[idx])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set 제작을 위한 class\n",
    "class svd_test_set(Dataset):\n",
    "    def __init__(self,data_path_list,classes,transform=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list\n",
    "        self.label = svd_test_set.get_label(self.path_list)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list):\n",
    "        label_list=[]\n",
    "        \n",
    "        for idx,x in enumerate(data_path_list):\n",
    "            label_list.append(Y_test[idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다. \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc, spectro, mel-spectro를 추출\n",
    "        2. mfcc를 300프레임으로 패딩. 또한 세로축은 128으로 interpolate\n",
    "        3. resnet에 사용되기 위해 3채널로 쌓기.\n",
    "        4. 미정. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig =  phrase_dict[self.path_list[idx]] # 16000hz 실시\n",
    "        \n",
    "        \n",
    "        MFCCs = librosa.feature.mfcc(y=sig, sr=sr,win_length=win_length ,n_fft=n_fft, hop_length=hop_length, n_mfcc=128)\n",
    "        #MFCCs = librosa.util.normalize(MFCCs) # l-infinity norm\n",
    "        #MFCCs=cv2.resize(MFCCs,(MFCCs.shape[1],128),interpolation=cv2.INTER_LINEAR)# interpolate 적용해서 128 사이즈로\n",
    "        \n",
    "        \n",
    "        \n",
    "        stft = librosa.stft(sig, win_length=win_length,n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        mel_feature = librosa.feature.melspectrogram(y=sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        #stft 300 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "\n",
    "        magnitude = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(magnitude)\n",
    "        #log_spectrogram = librosa.util.normalize(log_spectrogram) # l-infinity norm\n",
    "        \n",
    "        \n",
    "        #padding\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        log_spectrogram = pad2d(log_spectrogram, length)\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        MFCCs = pad2d(MFCCs, length) # mfcc 대신 encoder를 가져와서 해보자.\n",
    "        log_spectrogram=log_spectrogram[:128,:]# 224 x 300 으로 사이즈 조절\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            log_spectrogram=self.transform(log_spectrogram).type(torch.float32)# 타입 변화\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 타입 변화\n",
    "            MFCCs=self.transform(MFCCs).type(torch.float32)# 타입 변화\n",
    "            \n",
    "            \n",
    "            MSF = torch.stack([log_spectrogram,mel_feature,MFCCs])# 3채널로 복사.\n",
    "            MSF = MSF.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            ##################안쓰는 곳\n",
    "            log_spectrogram = torch.from_numpy(log_spectrogram).type(torch.float32)\n",
    "            log_spectrogram=log_spectrogram.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return MSF, self.classes.index(self.label[idx])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05129d",
   "metadata": {},
   "source": [
    "# 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89052fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 하이퍼 파라미터\n",
    "BATCH_SIZE =  32 #한 배치당 32개 음성데이터\n",
    "EPOCHS = 40 # 전체 데이터 셋을 50번 반복\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"augment\":\"No\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba97b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA LOADER 함수가 BATCH_size 단위로 분리해 지정.\n",
    "\n",
    "#확인을 위해 데이터셋 하나만 확인\n",
    "#transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                           svd_dataset(\n",
    "                                               X_train_list,\n",
    "                                               classes,\n",
    "                                               transform=transforms.ToTensor(),#이걸 composed로 고쳐서 전처리 하도록 수정.\n",
    "                                               data_num=0,\n",
    "                                               training=True\n",
    "                                           ),\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True,\n",
    "                                           ) # 순서가 암기되는것을 막기위해.\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                           svd_dataset(\n",
    "                                               X_valid_list,\n",
    "                                               classes,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               data_num=0,\n",
    "                                               training=False\n",
    "                                           ),\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True,) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9761d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로더.\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_test_set(\n",
    "                                                   X_test,\n",
    "                                                   classes,\n",
    "                                                   transform = transforms.ToTensor(),\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b15a86",
   "metadata": {},
   "source": [
    "# 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f866237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 데이터 확인하기\n",
    "for (X_train,Y_train) in train_loader:\n",
    "    print(\"X_train : \",X_train.size(),'type:',X_train.type())\n",
    "    print(\"Y_train : \",Y_train.size(),'type:',Y_train.type())\n",
    "    break\n",
    "    \n",
    "print(Y_train[0])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(X_train[0].view(128,300,3).numpy())\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylim((0,128))\n",
    "plt.ylabel(\"Frequency\")\n",
    "#plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram (dB)\")\n",
    "#batch: 32 / 3채널 / frame수: 500  /  feature수: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22994f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14521024",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_train[0][0].view(128,300).numpy(),sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e76e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_train[0][1].view(128,300).numpy(),sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_train[0][2].view(128,300).numpy(),sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valiation set 확인\n",
    "for (X_valid,Y_valid) in validation_loader:\n",
    "    print(\"X_valid : \",X_valid.size(),'type:',X_valid.type())\n",
    "    print(\"Y_valid : \",Y_valid.size(),'type:',Y_valid.type())\n",
    "    break\n",
    "\n",
    "print(Y_valid[0])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(X_valid[0].view(128,300,3).numpy())\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim((0,128))\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "#batch: 32 / 3채널 / frame수: 500  /  feature수: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a38987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valiation set 확인\n",
    "for (test_data,test_label) in validation_loader:\n",
    "    print(\"X_valid : \",test_data.size(),'type:',test_data.type())\n",
    "    print(\"Y_valid : \",test_label.size(),'type:',test_label.type())\n",
    "    break\n",
    "\n",
    "print(test_label[0])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(test_data[0].view(128,300,3).numpy())\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim((0,128))\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "#batch: 32 / 3채널 / frame수: 500  /  feature수: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec40ea4",
   "metadata": {},
   "source": [
    "# RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "# pretrained\n",
    "\n",
    "\n",
    "def model_initialize():\n",
    "    model = models.resnet18(pretrained=True).cuda()\n",
    "    model.ftrs = model.fc.in_features # in_features : fully connected의 입력수.\n",
    "    num_ftrs = model.fc.in_features\n",
    "\n",
    "    model.fc = nn.Sequential(nn.Linear(num_ftrs, 64),\n",
    "                             nn.BatchNorm1d(64),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(64,50),\n",
    "                             nn.BatchNorm1d(50),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(50,2)\n",
    "                            )\n",
    "\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "model=model_initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelSmoothingCrossEntropy,self).__init__()\n",
    "    def forward(self,x,target,smoothing=0.1):\n",
    "        confidence =1.-smoothing\n",
    "        logprobs = F.log_softmax(x,dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1,index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + smoothing*smooth_loss\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab03234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = LabelSmoothingCrossEntropy()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47749e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the model summary\n",
    "from torchsummary import summary\n",
    "#summary(model, input_size=(3, 128, 300), device=DEVICE.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 학습\n",
    "def train(model,train_loader,optimizer, log_interval):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx,(image,label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        #데이터들 장비에 할당\n",
    "        optimizer.zero_grad() # device 에 저장된 gradient 제거\n",
    "        output = model(image) # model로 output을 계산\n",
    "        loss = criterion(output, label) #loss 계산\n",
    "        #loss = softXEnt(output,label)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        output = F.softmax(output, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함\n",
    "        prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()# 아웃풋이 배치 사이즈 32개라서.\n",
    "        loss.backward() # loss 값을 이용해 gradient를 계산\n",
    "        optimizer.step() # Gradient 값을 이용해 파라미터 업데이트.\n",
    "    train_loss/=len(train_loader.dataset)\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss,train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09341bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. 학습 진행하며, validation 데이터로 모델 성능확인\n",
    "def evaluate(model,valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    #no_grad : 그래디언트 값 계산 막기.\n",
    "    with torch.no_grad():\n",
    "        for image, label in valid_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            \n",
    "            valid_loss += criterion(output, label).item()\n",
    "            #valid_loss +=softXEnt(output,label).item()\n",
    "            \n",
    "            output = F.softmax(output, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함.)\n",
    "            prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()# 아웃풋이 배치 사이즈 32개라서.\n",
    "            #true.false값을 sum해줌. item\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "        return valid_loss,valid_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fc2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로더 제작 함수\n",
    "\n",
    "def load_data(data_ind):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_train_list,\n",
    "                                                   classes,\n",
    "                                                   transform = transforms.ToTensor(),#이걸 composed로 고쳐서 전처리 하도록 수정.\n",
    "                                                   data_num=data_ind,\n",
    "                                                   training=True\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,\n",
    "                                               ) # 순서가 암기되는것을 막기위해.\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_valid_list,\n",
    "                                                   classes,\n",
    "                                                   transform = transforms.ToTensor(),\n",
    "                                                   data_num=data_ind,\n",
    "                                                   training=False\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) \n",
    "    return train_loader,validation_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8c86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10. 학습 및 평가.\n",
    "# resnet34 pretrained true\n",
    "# kfold 적용\n",
    "\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for data_ind in range(1,6):\n",
    "\n",
    "    check_path = '../../../checkpoint/checkpoint_spectro_resnet18_true_ros_'+str(data_ind)+'_300_msf_organic_base.pt'\n",
    "    print(check_path)\n",
    "    early_stopping = EarlyStopping(patience = 5, verbose = True, path=check_path)\n",
    "    train_loader,validation_loader = load_data(data_ind-1)\n",
    "    \n",
    "    best_train_acc=0 # accuracy 기록용\n",
    "    best_valid_acc=0\n",
    "    \n",
    "    model=model_initialize()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "    \n",
    "    \n",
    "    print(\"[{} 교차검증] 학습 시작\\n ----- \".format(data_ind))\n",
    "    for Epoch in range(1,EPOCHS+1):\n",
    "        train_loss,train_accuracy=train(model,train_loader,optimizer,log_interval=31)\n",
    "        valid_loss,valid_accuracy = evaluate(model, validation_loader)\n",
    "        wandb.log({\n",
    "                \"valid {}fold Accuracy\".format(data_ind) : valid_accuracy,\n",
    "                \"valid {}fold loss\".format(data_ind) : valid_loss})\n",
    "\n",
    "        print(\"\\n[EPOCH:{}]\\t Train Loss:{:.4f}\\t Train Acc:{:.2f} %  | \\tValid Loss:{:.4f} \\tValid Acc: {:.2f} %\\n\".\n",
    "              format(Epoch,train_loss,train_accuracy,valid_loss,valid_accuracy))\n",
    "        \n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "        if -early_stopping.best_score == valid_loss:\n",
    "            best_train_acc, best_valid_acc = train_accuracy,valid_accuracy\n",
    "            wandb.run.summary.update({\"best_valid_{}fold_acc\".format(data_ind) : best_valid_acc})\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "                train_accs.append(best_train_acc)\n",
    "                valid_accs.append(best_valid_acc)\n",
    "                print(\"[{} 교차검증] Early stopping\".format(data_ind))\n",
    "                break\n",
    "\n",
    "        if Epoch==EPOCHS:\n",
    "            #만약 early stop 없이 40 epoch라서 중지 된 경우.\n",
    "            train_accs.append(best_train_acc)\n",
    "            valid_accs.append(best_valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969079c",
   "metadata": {},
   "source": [
    "# Model 결과 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ef002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sum_valid=0\n",
    "for data_ind in range(5):\n",
    "    print(\"[{} 교차검증] train ACC : {:.4f} |\\t valid ACC: {:.4f} \".format(data_ind+1,train_accs[data_ind],valid_accs[data_ind] ))\n",
    "    sum_valid+=valid_accs[data_ind]\n",
    "    \n",
    "print(\"평균 검증 정확도\",sum_valid/5,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d3f46",
   "metadata": {},
   "source": [
    "# Model Test\n",
    "\n",
    "- test set\n",
    "- confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix 계산\n",
    "#test set 계산.\n",
    "def test_evaluate(model,test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    #no_grad : 그래디언트 값 계산 막기.\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "            answers +=label\n",
    "            predictions +=prediction\n",
    "            \n",
    "        return predictions,answers,test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446269f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix (resnet18)\n",
    "# kfold의 confusion matrix는 계산 방법이 다르다.\n",
    "# 모델을 각각 불러와서 test set을 평가한다.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cf = np.zeros((2,2))\n",
    "cf_list = []\n",
    "average_accuracy = 0\n",
    "average_fscore = 0\n",
    "average_uar = 0\n",
    "\n",
    "for data_ind in range(1,6):\n",
    "\n",
    "    check_path = '../../../checkpoint/checkpoint_spectro_resnet18_true_ros_'+str(data_ind)+'_300_msf_organic_base.pt'\n",
    "    model.load_state_dict(torch.load(check_path))\n",
    "\n",
    "    predictions,answers,test_loss = test_evaluate(model, test_loader)\n",
    "    predictions=[ dat.cpu().numpy() for dat in predictions]\n",
    "    answers=[ dat.cpu().numpy() for dat in answers]\n",
    "\n",
    "    \n",
    "    cf = confusion_matrix(answers, predictions)\n",
    "    cf_list.append(cf)\n",
    "    \n",
    "    acc = (cf[0,0]+cf[1,1])/(cf[0,0]+cf[0,1]+cf[1,0]+cf[1,1])\n",
    "    average_accuracy+=acc\n",
    "    \n",
    "    precision=cf[0,0]/(cf[0,0]+cf[1,0])\n",
    "    recall=cf[0,0]/(cf[0,0]+cf[1,0])\n",
    "    \n",
    "    specificity=cf[1,1]/(cf[0,1]+cf[1,1])\n",
    "    average_uar += (specificity+recall)/2\n",
    "    #fscore=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    #fscroe macro추가\n",
    "    fscore = f1_score(answers,predictions,average='macro')\n",
    "    average_fscore+=fscore\n",
    "    \n",
    "    print('{}번 모델'.format(data_ind))\n",
    "    print(\"Accuracy : {:.4f}% \".format(acc*100))\n",
    "    #print(\"Precision (pathology 예측한 것중 맞는 것) : {:.4f}\".format(precision))\n",
    "    print(\"recall (실제 pathology 중  예측이 맞는 것) : {:.4f}\".format(recall))\n",
    "    print(\"specificity : {:.4f}%\".format(specificity))\n",
    "    print(\"UAR : {:.4f}%\".format( (specificity+recall)/2 ))\n",
    "    \n",
    "    \n",
    "    print(\"f score : {:.4f} \".format(fscore))\n",
    "    print(cf)\n",
    "    print(\"-----\")\n",
    "    #### wandb\n",
    "    \n",
    "    wandb.run.summary.update({\"test_{}fold_acc\".format(data_ind) : acc*100})\n",
    "    wandb.run.summary.update({\"test_{}fold_f1\".format(data_ind) : fscore})\n",
    "    wandb.run.summary.update({\"test_{}fold_UAR\".format(data_ind) : (specificity+recall)/2})\n",
    "    wandb.log({\"{}fold Confusion Matrix\".format(data_ind) :wandb.sklearn.plot_confusion_matrix(answers, predictions, labels=classes)})\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(\"평균 acc : {:.4f}\".format(average_accuracy/5))\n",
    "print(\"평균 UAR : {:.4f}\".format(average_uar/5))\n",
    "print(\"평균 f1score : {:.4f}\".format(average_fscore/5))\n",
    "wandb.run.summary.update({\"test 평균 acc\" : average_accuracy/5})\n",
    "wandb.run.summary.update({\"test 평균 f1\" : average_fscore/5})\n",
    "wandb.run.summary.update({\"test 평균 UAR\" : average_uar/5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c41999",
   "metadata": {},
   "source": [
    "## validation set 답 정리\n",
    "- 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#분포 확인 위해 수정\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=16000\n",
    "win_length =  np.int64(sr/40) # \n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "\n",
    "class svd_dataset(Dataset):\n",
    "    def __init__(self,data_path_list,classes,data_num,training,transform=None,augment=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list[data_num]\n",
    "        self.data_num = data_num\n",
    "        self.training = training\n",
    "        self.label = svd_dataset.get_label(self.path_list,training,data_num)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        self.augment=augment\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list,training,data_num):\n",
    "        label_list=[]\n",
    "        \n",
    "        if training:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_train_list[data_num][idx])\n",
    "        else:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_valid_list[data_num][idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다.     \n",
    "    \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc, spectro, mel-spectro를 추출\n",
    "        2. mfcc를 224프레임으로 패딩. 또한 세로축은 224으로 interpolate\n",
    "        3. resnet에 사용되기 위해 3채널로 쌓기.\n",
    "        4. 미정. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig =  phrase_dict[self.path_list[idx]] # 16000hz 실시\n",
    "        \n",
    "        \n",
    "        MFCCs = librosa.feature.mfcc(y=sig, sr=sr,win_length=win_length ,n_fft=n_fft, hop_length=hop_length, n_mfcc=128)\n",
    "        #MFCCs = librosa.util.normalize(MFCCs) # l-infinity norm\n",
    "        #MFCCs=cv2.resize(MFCCs,(MFCCs.shape[1],128),interpolation=cv2.INTER_LINEAR)# interpolate 적용해서 128 사이즈로\n",
    "        \n",
    "        \n",
    "        \n",
    "        stft = librosa.stft(sig, win_length=win_length,n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        mel_feature = librosa.feature.melspectrogram(y=sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        #stft 300 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "\n",
    "        magnitude = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(magnitude)\n",
    "        #log_spectrogram = librosa.util.normalize(log_spectrogram) # l-infinity norm\n",
    "        \n",
    "        \n",
    "        #padding\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        log_spectrogram = pad2d(log_spectrogram, length)\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        MFCCs = pad2d(MFCCs, length) # mfcc 대신 encoder를 가져와서 해보자.\n",
    "        log_spectrogram=log_spectrogram[:128,:]# 224 x 300 으로 사이즈 조절\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            log_spectrogram=self.transform(log_spectrogram).type(torch.float32)# 타입 변화\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 타입 변화\n",
    "            MFCCs=self.transform(MFCCs).type(torch.float32)# 타입 변화            \n",
    "            \n",
    "            MSF = torch.stack([log_spectrogram,mel_feature,MFCCs])# 3채널로 복사.\n",
    "            #augmentation\n",
    "            if self.training:\n",
    "                MSF=self.augment(MSF)     \n",
    "            MSF = MSF.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            ##################안쓰는 곳\n",
    "            log_spectrogram = torch.from_numpy(log_spectrogram).type(torch.float32)\n",
    "            log_spectrogram=log_spectrogram.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return MSF, self.classes.index(self.label[idx]),self.path_list[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix 계산\n",
    "#valid set 계산.\n",
    "all_filename=[]\n",
    "all_prediction=[]\n",
    "all_answers=[]\n",
    "\n",
    "\n",
    "def valid_evaluate(model,data_ind):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    file_name = []\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    #no_grad : 그래디언트 값 계산 막기.\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_valid_list,\n",
    "                                                   classes,\n",
    "                                                   transform=transforms.ToTensor(),\n",
    "                                                   data_num=data_ind-1,\n",
    "                                                   training=False\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label,path_list in validation_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "            answers +=label\n",
    "            predictions +=prediction\n",
    "            file_name+=(path_list)            \n",
    "        all_filename.append(file_name)\n",
    "        all_prediction.append(predictions)\n",
    "        all_answers.append(answers)\n",
    "        return predictions,answers,test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (resnet18)\n",
    "# kfold의 confusion matrix는 계산 방법이 다르다.\n",
    "# 모델을 각각 불러와서 test set을 평가한다.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cf = np.zeros((2,2))\n",
    "cf_list = []\n",
    "average_accuracy = 0\n",
    "average_fscore = 0\n",
    "\n",
    "for data_ind in range(1,6):\n",
    "\n",
    "    check_path = '../../../checkpoint/checkpoint_spectro_resnet18_true_ros_'+str(data_ind)+'_300_msf_organic_aug.pt'\n",
    "    model.load_state_dict(torch.load(check_path))\n",
    "\n",
    "    predictions,answers, valid_loss = valid_evaluate(model,data_ind)\n",
    "    predictions=[ dat.cpu().numpy() for dat in predictions]\n",
    "    answers=[ dat.cpu().numpy() for dat in answers]\n",
    "\n",
    "    \n",
    "    cf = confusion_matrix(answers, predictions)\n",
    "    cf_list.append(cf)\n",
    "    \n",
    "    acc = (cf[0,0]+cf[1,1])/(cf[0,0]+cf[0,1]+cf[1,0]+cf[1,1])\n",
    "    average_accuracy+=acc\n",
    "    precision=cf[0,0]/(cf[0,0]+cf[1,0])\n",
    "    recall=cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "    #fscore=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    #fscroe macro추가\n",
    "    fscore = f1_score(answers,predictions,average='macro')\n",
    "    average_fscore+=fscore\n",
    "    \n",
    "    print('{}번 모델'.format(data_ind))\n",
    "    print(\"Accuracy : {:.4f}% \".format(acc*100))\n",
    "    print(\"Precision (pathology 예측한 것중 맞는 것) : {:.4f}\".format(precision))\n",
    "    print(\"recall (실제 pathology 중  예측이 맞는 것) : {:.4f}\".format(recall))\n",
    "    print(\"f score : {:.4f} \".format(fscore))\n",
    "    print(cf)\n",
    "    print(\"-----\")\n",
    "\n",
    "print(\"평균 acc : {:.4f}\".format(average_accuracy/5))\n",
    "print(\"평균 f1score : {:.4f}\".format(average_fscore/5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_excel = []\n",
    "for i in range(5):\n",
    "    fold_excel.append(pd.DataFrame({'filename':all_filename[i],\n",
    "                  'prediction':[data.cpu().numpy().item() for data in all_prediction[i]],\n",
    "                  'answer':[ data.cpu().numpy().item() for data in all_answers[i]],\n",
    "                  'fold':i+1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a378f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_excel_all=pd.concat(fold_excel,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper=pd.read_excel('D:/project/voice_pathology_ai/voice_data/only_organics_healthy.xlsx')\n",
    "answer_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dae7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper['RECORDING']=answer_paper['RECORDING'].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper['RECORDING']=answer_paper['RECORDING']+'-phrase.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58875f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper[['RECORDING','DETAIL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ff604",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left = pd.merge(fold_excel_all,answer_paper[['RECORDING','DETAIL']], how='left', left_on='filename', right_on='RECORDING')\n",
    "merge_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db892e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left.drop(['RECORDING'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37905a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left['result']=merge_left['prediction']==merge_left['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left.to_excel('D:/project/voice_pathology_ai/voice_data/result_valid_organics_0523_aug.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c96e56",
   "metadata": {},
   "source": [
    "## validation cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final conv layer name \n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# activations\n",
    "feature_blobs = []\n",
    "\n",
    "# gradient를 가져올 hook 함수\n",
    "backward_feature = []\n",
    "\n",
    "transform_norm = transforms.ToTensor()\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy()) # 레이어의 마지막 output(피처맵)을 구하는 함수\n",
    "    \n",
    "\n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])  #backward시에 그래디언트 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c713d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#분포 확인 위해 수정\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=16000\n",
    "win_length =  np.int64(sr/40) # \n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "\n",
    "class svd_dataset(Dataset):\n",
    "    def __init__(self,data_path_list,classes,data_num,training,transform=None,augment=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list[data_num]\n",
    "        self.data_num = data_num\n",
    "        self.training = training\n",
    "        self.label = svd_dataset.get_label(self.path_list,training,data_num)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        self.augment=augment\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list,training,data_num):\n",
    "        print()\n",
    "        label_list=[]\n",
    "        \n",
    "        if training:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_train_list[data_num][idx])\n",
    "        else:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_valid_list[data_num][idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다.     \n",
    "    \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc, spectro, mel-spectro를 추출\n",
    "        2. mfcc를 224프레임으로 패딩. 또한 세로축은 224으로 interpolate\n",
    "        3. resnet에 사용되기 위해 3채널로 쌓기.\n",
    "        4. 미정. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig =  phrase_dict[self.path_list[idx]] # 16000hz 실시\n",
    "        \n",
    "        \n",
    "        MFCCs = librosa.feature.mfcc(y=sig, sr=sr,win_length=win_length ,n_fft=n_fft, hop_length=hop_length, n_mfcc=128)\n",
    "        #MFCCs = librosa.util.normalize(MFCCs) # l-infinity norm\n",
    "        #MFCCs=cv2.resize(MFCCs,(MFCCs.shape[1],128),interpolation=cv2.INTER_LINEAR)# interpolate 적용해서 128 사이즈로\n",
    "        \n",
    "        \n",
    "        \n",
    "        stft = librosa.stft(sig, win_length=win_length,n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        mel_feature = librosa.feature.melspectrogram(y=sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        #stft 300 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "\n",
    "        magnitude = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(magnitude)\n",
    "        #log_spectrogram = librosa.util.normalize(log_spectrogram) # l-infinity norm\n",
    "        \n",
    "        \n",
    "        #padding\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        log_spectrogram = pad2d(log_spectrogram, length)\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        MFCCs = pad2d(MFCCs, length) # mfcc 대신 encoder를 가져와서 해보자.\n",
    "        log_spectrogram=log_spectrogram[:128,:]# 224 x 300 으로 사이즈 조절\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            log_spectrogram=self.transform(log_spectrogram).type(torch.float32)# 타입 변화\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 타입 변화\n",
    "            MFCCs=self.transform(MFCCs).type(torch.float32)# 타입 변화            \n",
    "            \n",
    "            MSF = torch.stack([log_spectrogram,mel_feature,MFCCs])# 3채널로 복사.\n",
    "            #augmentation\n",
    "            if self.training:\n",
    "                MSF=self.augment(MSF)     \n",
    "            MSF = MSF.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            ##################안쓰는 곳\n",
    "            log_spectrogram = torch.from_numpy(log_spectrogram).type(torch.float32)\n",
    "            log_spectrogram=log_spectrogram.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return MSF, self.classes.index(self.label[idx]), self.path_list[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAM_BATCH_SIZE=1\n",
    "finalconv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f9c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "\n",
    "\n",
    "#pathology 음성 파일 가져오기\n",
    "\n",
    "length=300\n",
    "image_list=[]\n",
    "\n",
    "label_list=[]\n",
    "prob_list=[] #valid set의 확률값\n",
    "name_list=[] #파일명\n",
    "predict_label_list=[] #예측 라벨\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "mel_list=[]#정규 mel-spectrogram 모은 리스트\n",
    "fallout_list=[] # cam - mel . 음수는 0 처리.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "mel_sample=None\n",
    "\n",
    "\n",
    "for data_ind in  range(1,6):\n",
    "    print(\"----------------\")\n",
    "    print(data_ind,'fold')\n",
    "    print(\"----------------\")\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_valid_list,\n",
    "                                                   classes,\n",
    "                                                   transform=transforms.ToTensor(),\n",
    "                                                   data_num=data_ind-1,\n",
    "                                                   training=False\n",
    "                                               ),\n",
    "                                               batch_size = CAM_BATCH_SIZE,\n",
    "                                               shuffle = True,) \n",
    "    for image, label, path in validation_loader:\n",
    "\n",
    "        name = path[0].split('\\\\')[-1]\n",
    "        name = name.replace('.wav', '')\n",
    "        name_list.append(name)\n",
    "        label_list.append(classes[label])\n",
    "\n",
    "        # activations\n",
    "        feature_blobs = []\n",
    "        # gradient를 가져올 hook 함수\n",
    "        backward_feature = []\n",
    "\n",
    "        mel_sample = image\n",
    "        mel_sample = mel_sample.to(DEVICE).float()\n",
    "\n",
    "\n",
    "        model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "        model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "\n",
    "        # Prediction\n",
    "        logit = model(mel_sample) # 예측값 구하기.\n",
    "        out = F.softmax(logit, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함.)\n",
    "        probs, idx = out.sort(0, True)\n",
    "        print(\"Predicted label : %d, Probability : %.2f\" % (idx[0].item(), probs[0].item()))\n",
    "        predict_label_list.append(classes[idx[0].item()])\n",
    "        prob_list.append(probs[0].item())\n",
    "        res.append(classes[label]==classes[idx[0].item()])\n",
    "\n",
    "\n",
    "        ###########\n",
    "        # Grad - cam\n",
    "        ###########\n",
    "\n",
    "        score = logit[:, idx[0]].squeeze() # 예측값 y^c.\n",
    "        score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "\n",
    "        activations = torch.Tensor(feature_blobs[0]).to(DEVICE) # (1, 512, 7, 7), forward activations. append라서 0번이 마지막\n",
    "        gradients = backward_feature[0] # (1, 512, 4, 10), backward gradients. 마지막 conv layer의 gradient\n",
    "        b, k, u, v = gradients.size()  # batch, 피처맵 수,  상, 하\n",
    "        #print(gradients.size())\n",
    "\n",
    "        alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "        weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "\n",
    "        grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "        grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "        grad_cam_map = F.interpolate(grad_cam_map, size=(128, 300), mode='bilinear', align_corners=False) # (1, 1, 128, 300)\n",
    "        map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "        grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 128, 300), min-max scaling\n",
    "\n",
    "        grad_heatmap=grad_cam_map.squeeze().cpu()\n",
    "\n",
    "        # grad_cam_map.squeeze() : (128, 300)\n",
    "\n",
    "        #save_image(grad_heatmap, os.path.join(\"./\", \"Grad_CAM.jpg\"))\n",
    "\n",
    "        # mel-spectorgram min-max normalization\n",
    "\n",
    "        mel_min,mel_max = mel_sample.squeeze()[1].min(),mel_sample.squeeze()[1].max()\n",
    "        mel_sample = (mel_sample.squeeze()[1]- mel_min).div(mel_max-mel_min)\n",
    "        mel_sample = mel_sample.cpu().numpy()\n",
    "        mel_list.append(mel_sample)\n",
    "\n",
    "\n",
    "        #mel_sample = librosa.util.normalize(mel_sample.cpu().squeeze().numpy()[0]) #수정 필요. min-max normalization\n",
    "\n",
    "        grad_result = grad_heatmap.numpy() + mel_sample # (1, 3, 244, 244)\n",
    "\n",
    "        fallout_list.append(grad_heatmap.numpy() - mel_sample)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.ion()\n",
    "        plt.subplot(131)\n",
    "        librosa.display.specshow(mel_sample, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "        plt.subplot(132)\n",
    "        librosa.display.specshow(grad_heatmap.numpy(), sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "        plt.subplot(133)\n",
    "        librosa.display.specshow(grad_result, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "\n",
    "        \n",
    "        folder_path='D:/project/voice_pathology_ai/voice_data/aug_organics_cam_fold/'+str(data_ind)+'fold/'\n",
    "        plt.ioff()\n",
    "        plt.savefig(folder_path+name)\n",
    "    \n",
    "\n",
    "result_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(predict_label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "result_excel.columns=['name','class','predict','probability','result']\n",
    "result_excel.to_excel(\"D:/project/voice_pathology_ai/voice_data/result_excel_fallout_valid.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ecb67",
   "metadata": {},
   "source": [
    "# train set 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254502ab",
   "metadata": {},
   "source": [
    "## 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix 계산\n",
    "#valid set 계산.\n",
    "all_filename=[]\n",
    "all_prediction=[]\n",
    "all_answers=[]\n",
    "\n",
    "\n",
    "def train_evaluate(model,data_ind):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    file_name = []\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    #no_grad : 그래디언트 값 계산 막기.\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_train_list,\n",
    "                                                   classes,\n",
    "                                                   transform=transforms.ToTensor(),\n",
    "                                                   data_num=data_ind-1,\n",
    "                                                   training=True\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label,path_list in train_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "            answers +=label\n",
    "            predictions +=prediction\n",
    "            file_name+=(path_list)            \n",
    "        all_filename.append(file_name)\n",
    "        all_prediction.append(predictions)\n",
    "        all_answers.append(answers)\n",
    "        return predictions,answers,test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#분포 확인 위해 수정\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=16000\n",
    "win_length =  np.int64(sr/40) # \n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "\n",
    "class svd_dataset(Dataset):\n",
    "    def __init__(self,data_path_list,classes,data_num,training,transform=None,augment=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list[data_num]\n",
    "        self.data_num = data_num\n",
    "        self.training = training\n",
    "        self.label = svd_dataset.get_label(self.path_list,training,data_num)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        self.augment=augment\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list,training,data_num):\n",
    "        print()\n",
    "        label_list=[]\n",
    "        \n",
    "        if training:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_train_list[data_num][idx])\n",
    "        else:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_valid_list[data_num][idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다.     \n",
    "    \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc, spectro, mel-spectro를 추출\n",
    "        2. mfcc를 224프레임으로 패딩. 또한 세로축은 224으로 interpolate\n",
    "        3. resnet에 사용되기 위해 3채널로 쌓기.\n",
    "        4. 미정. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig =  phrase_dict[self.path_list[idx]] # 16000hz 실시\n",
    "        \n",
    "        \n",
    "        MFCCs = librosa.feature.mfcc(y=sig, sr=sr,win_length=win_length ,n_fft=n_fft, hop_length=hop_length, n_mfcc=128)\n",
    "        #MFCCs = librosa.util.normalize(MFCCs) # l-infinity norm\n",
    "        #MFCCs=cv2.resize(MFCCs,(MFCCs.shape[1],128),interpolation=cv2.INTER_LINEAR)# interpolate 적용해서 128 사이즈로\n",
    "        \n",
    "        \n",
    "        \n",
    "        stft = librosa.stft(sig, win_length=win_length,n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        mel_feature = librosa.feature.melspectrogram(y=sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        #stft 300 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "\n",
    "        magnitude = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(magnitude)\n",
    "        #log_spectrogram = librosa.util.normalize(log_spectrogram) # l-infinity norm\n",
    "        \n",
    "        \n",
    "        #padding\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        log_spectrogram = pad2d(log_spectrogram, length)\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        MFCCs = pad2d(MFCCs, length) # mfcc 대신 encoder를 가져와서 해보자.\n",
    "        log_spectrogram=log_spectrogram[:128,:]# 224 x 300 으로 사이즈 조절\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            log_spectrogram=self.transform(log_spectrogram).type(torch.float32)# 타입 변화\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 타입 변화\n",
    "            MFCCs=self.transform(MFCCs).type(torch.float32)# 타입 변화            \n",
    "            \n",
    "            MSF = torch.stack([log_spectrogram,mel_feature,MFCCs])# 3채널로 복사.\n",
    "            #augmentation\n",
    "            if self.training and self.augment:\n",
    "                MSF=self.augment(MSF)     \n",
    "            MSF = MSF.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            ##################안쓰는 곳\n",
    "            log_spectrogram = torch.from_numpy(log_spectrogram).type(torch.float32)\n",
    "            log_spectrogram=log_spectrogram.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return MSF, self.classes.index(self.label[idx]), self.path_list[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (resnet18)\n",
    "# kfold의 confusion matrix는 계산 방법이 다르다.\n",
    "# 모델을 각각 불러와서 test set을 평가한다.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cf = np.zeros((2,2))\n",
    "cf_list = []\n",
    "average_accuracy = 0\n",
    "average_fscore = 0\n",
    "\n",
    "for data_ind in range(1,6):\n",
    "\n",
    "    check_path = '../../../checkpoint/checkpoint_spectro_resnet18_true_ros_'+str(data_ind)+'_300_msf_organic_aug.pt'\n",
    "    model.load_state_dict(torch.load(check_path))\n",
    "\n",
    "    predictions,answers, valid_loss = train_evaluate(model,data_ind)\n",
    "    predictions=[ dat.cpu().numpy() for dat in predictions]\n",
    "    answers=[ dat.cpu().numpy() for dat in answers]\n",
    "\n",
    "    \n",
    "    cf = confusion_matrix(answers, predictions)\n",
    "    cf_list.append(cf)\n",
    "    \n",
    "    acc = (cf[0,0]+cf[1,1])/(cf[0,0]+cf[0,1]+cf[1,0]+cf[1,1])\n",
    "    average_accuracy+=acc\n",
    "    precision=cf[0,0]/(cf[0,0]+cf[1,0])\n",
    "    recall=cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "    #fscore=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    #fscroe macro추가\n",
    "    fscore = f1_score(answers,predictions,average='macro')\n",
    "    average_fscore+=fscore\n",
    "    \n",
    "    print('{}번 모델'.format(data_ind))\n",
    "    print(\"Accuracy : {:.4f}% \".format(acc*100))\n",
    "    print(\"Precision (pathology 예측한 것중 맞는 것) : {:.4f}\".format(precision))\n",
    "    print(\"recall (실제 pathology 중  예측이 맞는 것) : {:.4f}\".format(recall))\n",
    "    print(\"f score : {:.4f} \".format(fscore))\n",
    "    print(cf)\n",
    "    print(\"-----\")\n",
    "\n",
    "print(\"평균 acc : {:.4f}\".format(average_accuracy/5))\n",
    "print(\"평균 f1score : {:.4f}\".format(average_fscore/5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_excel = []\n",
    "for i in range(5):\n",
    "    fold_excel.append(pd.DataFrame({'filename':all_filename[i],\n",
    "                  'prediction':[data.cpu().numpy().item() for data in all_prediction[i]],\n",
    "                  'answer':[ data.cpu().numpy().item() for data in all_answers[i]],\n",
    "                  'fold':i+1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_excel_all=pd.concat(fold_excel,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper=pd.read_excel('D:/project/voice_pathology_ai/voice_data/only_organics_healthy.xlsx')\n",
    "answer_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a447e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_paper['RECORDING']=answer_paper['RECORDING'].values.astype(str)\n",
    "answer_paper['RECORDING']=answer_paper['RECORDING']+'-phrase.wav'\n",
    "answer_paper[['RECORDING','DETAIL']]\n",
    "merge_left = pd.merge(fold_excel_all,answer_paper[['RECORDING','DETAIL']], how='left', left_on='filename', right_on='RECORDING')\n",
    "merge_left.drop(['RECORDING'],axis=1,inplace=True)\n",
    "merge_left['result']=merge_left['prediction']==merge_left['answer']\n",
    "merge_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_left.to_excel('D:/project/voice_pathology_ai/voice_data/result_train_organics_0523_aug.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b4901",
   "metadata": {},
   "source": [
    "## train cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final conv layer name \n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# activations\n",
    "feature_blobs = []\n",
    "\n",
    "# gradient를 가져올 hook 함수\n",
    "backward_feature = []\n",
    "\n",
    "transform_norm = transforms.ToTensor()\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy()) # 레이어의 마지막 output(피처맵)을 구하는 함수\n",
    "    \n",
    "\n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])  #backward시에 그래디언트 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAM_BATCH_SIZE=1\n",
    "finalconv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7588f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "\n",
    "\n",
    "#pathology 음성 파일 가져오기\n",
    "\n",
    "length=300\n",
    "image_list=[]\n",
    "\n",
    "label_list=[]\n",
    "prob_list=[] #valid set의 확률값\n",
    "name_list=[] #파일명\n",
    "predict_label_list=[] #예측 라벨\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "mel_list=[]#정규 mel-spectrogram 모은 리스트\n",
    "fallout_list=[] # cam - mel . 음수는 0 처리.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "mel_sample=None\n",
    "\n",
    "\n",
    "for data_ind in  range(1,6):\n",
    "    print(\"----------------\")\n",
    "    print(data_ind,'fold')\n",
    "    print(\"----------------\")\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_dataset(\n",
    "                                                   X_train_list,\n",
    "                                                   classes,\n",
    "                                                   transform=transforms.ToTensor(),\n",
    "                                                   data_num=data_ind-1,\n",
    "                                                   training=True\n",
    "                                               ),\n",
    "                                               batch_size = CAM_BATCH_SIZE,\n",
    "                                               shuffle = True,) \n",
    "    for image, label, path in train_loader:\n",
    "\n",
    "        name = path[0].split('\\\\')[-1]\n",
    "        name = name.replace('.wav', '')\n",
    "        name_list.append(name)\n",
    "        label_list.append(classes[label])\n",
    "\n",
    "        # activations\n",
    "        feature_blobs = []\n",
    "        # gradient를 가져올 hook 함수\n",
    "        backward_feature = []\n",
    "\n",
    "        mel_sample = image\n",
    "        mel_sample = mel_sample.to(DEVICE).float()\n",
    "\n",
    "\n",
    "        model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "        model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "\n",
    "        # Prediction\n",
    "        logit = model(mel_sample) # 예측값 구하기.\n",
    "        out = F.softmax(logit, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함.)\n",
    "        probs, idx = out.sort(0, True)\n",
    "        print(\"Predicted label : %d, Probability : %.2f\" % (idx[0].item(), probs[0].item()))\n",
    "        predict_label_list.append(classes[idx[0].item()])\n",
    "        prob_list.append(probs[0].item())\n",
    "        res.append(classes[label]==classes[idx[0].item()])\n",
    "\n",
    "\n",
    "        ###########\n",
    "        # Grad - cam\n",
    "        ###########\n",
    "\n",
    "        score = logit[:, idx[0]].squeeze() # 예측값 y^c.\n",
    "        score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "\n",
    "        activations = torch.Tensor(feature_blobs[0]).to(DEVICE) # (1, 512, 7, 7), forward activations. append라서 0번이 마지막\n",
    "        gradients = backward_feature[0] # (1, 512, 4, 10), backward gradients. 마지막 conv layer의 gradient\n",
    "        b, k, u, v = gradients.size()  # batch, 피처맵 수,  상, 하\n",
    "        #print(gradients.size())\n",
    "\n",
    "        alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "        weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "\n",
    "        grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "        grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "        grad_cam_map = F.interpolate(grad_cam_map, size=(128, 300), mode='bilinear', align_corners=False) # (1, 1, 128, 300)\n",
    "        map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "        grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 128, 300), min-max scaling\n",
    "\n",
    "        grad_heatmap=grad_cam_map.squeeze().cpu()\n",
    "\n",
    "        # grad_cam_map.squeeze() : (128, 300)\n",
    "\n",
    "        #save_image(grad_heatmap, os.path.join(\"./\", \"Grad_CAM.jpg\"))\n",
    "\n",
    "        # mel-spectorgram min-max normalization\n",
    "\n",
    "        mel_min,mel_max = mel_sample.squeeze()[1].min(),mel_sample.squeeze()[1].max()\n",
    "        mel_sample = (mel_sample.squeeze()[1]- mel_min).div(mel_max-mel_min)\n",
    "        mel_sample = mel_sample.cpu().numpy()\n",
    "        mel_list.append(mel_sample)\n",
    "\n",
    "\n",
    "        #mel_sample = librosa.util.normalize(mel_sample.cpu().squeeze().numpy()[0]) #수정 필요. min-max normalization\n",
    "\n",
    "        grad_result = grad_heatmap.numpy() + mel_sample # (1, 3, 244, 244)\n",
    "\n",
    "        fallout_list.append(grad_heatmap.numpy() - mel_sample)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.ion()\n",
    "        plt.subplot(131)\n",
    "        librosa.display.specshow(mel_sample, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "        plt.subplot(132)\n",
    "        librosa.display.specshow(grad_heatmap.numpy(), sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "        plt.subplot(133)\n",
    "        librosa.display.specshow(grad_result, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "\n",
    "        \n",
    "        folder_path='D:/project/voice_pathology_ai/voice_data/aug_organics_cam_fold_train/'+str(data_ind)+'fold/'\n",
    "        plt.savefig(folder_path+name)\n",
    "        plt.close() \n",
    "        \n",
    "    \n",
    "\n",
    "result_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(predict_label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "result_excel.columns=['name','class','predict','probability','result']\n",
    "#result_excel.to_excel(\"D:/project/voice_pathology_ai/voice_data/result_excel_fallout_train.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89da3e",
   "metadata": {},
   "source": [
    "# 기타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02835a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크기 알아보기\n",
    "#일반 CNN\n",
    "\n",
    "m = nn.MaxPool2d(2, stride=2)\n",
    "#m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
    "input = torch.randn(32, 1, 500, 13)\n",
    "print(input.size())\n",
    "output = m(input)\n",
    "print(output.size())\n",
    "output = m(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크기 알아보기\n",
    "#주파수 영역 CNN\n",
    "\n",
    "input = torch.randn(32, 1, 500, 13)\n",
    "m=nn.Conv2d(\n",
    "            in_channels = 1,# 채널이 1개\n",
    "            out_channels = 32,# FeatureMap 수,커널 수\n",
    "            kernel_size = (1,13),#1x13  no padding\n",
    "        )\n",
    "output=m(input)\n",
    "print(output.size())\n",
    "m=nn.Conv2d(\n",
    "            in_channels = 32,# 채널이 1개\n",
    "            out_channels = 64,# FeatureMap 수,커널 수\n",
    "            kernel_size = (27,1),#9 x 1 no padding\n",
    "        )\n",
    "output=m(output)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23df4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(500, 13)\n",
    "\n",
    "out=torch.stack([input,input,input])\n",
    "out.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
