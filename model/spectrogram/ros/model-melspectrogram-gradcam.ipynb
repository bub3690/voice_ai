{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f378364c",
   "metadata": {},
   "source": [
    "# 실험 주제\n",
    "\n",
    "resnet18을 통해 분류된 데이터에서, grad-cam을 추출한다.\n",
    "\n",
    "grad-cam과 normalized mel-spectrogram을 더해서, 다시 resnet18을 train한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505136e",
   "metadata": {},
   "source": [
    "- http://keunwoochoi.blogspot.com/2016/03/2.html\n",
    "- http://www.rex-ai.info/docs/AI_Example_CNN_speech_recognize\n",
    "- https://www.youtube.com/watch?v=oltGIc4uo5c\n",
    "- https://youdaeng-com.tistory.com/5\n",
    "- https://quokkas.tistory.com/37 : early stopping\n",
    "- https://continuous-development.tistory.com/166 : stratified kfold\n",
    "- https://deep-learning-study.tistory.com/476 fiter 시각화\n",
    "- https://wyatt37.tistory.com/10 : random over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # 인공 신경망 모델들 모아놓은 모듈\n",
    "import torch.nn.functional as F #그중 자주 쓰이는것들을 F로\n",
    "from torchvision import transforms, datasets\n",
    "import cv2\n",
    "from torchvision import transforms, datasets\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "from glob import glob\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "p = os.path.abspath('../..') # 상위 폴더를 사용하기 위해서.\n",
    "sys.path.insert(1, p)\n",
    "from pytorchtools.pytorchtools import EarlyStopping # 상위 폴더에 추가된 모듈.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "#DEVICE = torch.device('cpu')\n",
    "print('Using Pytorch version : ',torch.__version__,' Device : ',DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ccbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa, librosa.display \n",
    "import matplotlib.pyplot as plt\n",
    "#window sizde : FFT를 할때 참조할 그래프 길이 ( 프레임 하나당 sample 수 )\n",
    "#자연어 처리에서는 25ms 사용. https://ahnjg.tistory.com/93\n",
    "#초당 50000hz 중 1250개씩 윈도우 사이즈로 사용.\n",
    "sr=50000\n",
    "win_length =  np.int64(50000/40) # 1250\n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ebea6",
   "metadata": {},
   "source": [
    "# SVD 문장 데이터에서 Feature 추출\n",
    "- mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38dcdc",
   "metadata": {},
   "source": [
    "# 데이터 나누기 - Stratified KFold\n",
    "\n",
    "- pathology : 1194 / healthy : 634 / 총 1828\n",
    "- k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a148977",
   "metadata": {},
   "source": [
    "## 1. test/ train 나누기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. train, test 나누기\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split # train , test 분리에 사용.\n",
    "\n",
    "\n",
    "pathology = glob('../../../voice_data/fusion/pathology/phrase/*.wav')\n",
    "healthy = glob('../../../voice_data/fusion/healthy/phrase/*.wav')\n",
    "print(\"Pathology : \",len(pathology))\n",
    "print(\"Healthy: \",len(healthy))\n",
    "\n",
    "X = pathology+healthy # path 데이터 합\n",
    "print(\"총 데이터수 : \",len(X))\n",
    "Y = [] # 라벨\n",
    "for idx,x in enumerate(X):\n",
    "    if idx<1193:\n",
    "        Y.append(\"pathology\")\n",
    "    else:\n",
    "        Y.append(\"healthy\")\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y, random_state=456)\n",
    "#stratify를 넣어서, test에도 라벨별 잘 분류되게 한다.\n",
    "\n",
    "print(\"---\")\n",
    "print(\"훈련 셋 : \",len(Y),Counter(Y))\n",
    "print(\"테스트 셋 : \",len(Y_test),Counter(Y_test))\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4959aa",
   "metadata": {},
   "source": [
    "# 2. train set 모두 mel-spectro , grad-cam 구해주기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a99443",
   "metadata": {},
   "source": [
    "## 2. random over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ceb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. random over sampling\n",
    "\n",
    "X = np.array(X).reshape(-1,1)#각 데이터를 다 행으로 넣음. (1194,1)\n",
    "#Y = np.array(Y)\n",
    "ros = RandomOverSampler(random_state = 123)\n",
    "X_res,Y_res = ros.fit_resample(X,Y)\n",
    "\n",
    "print('before dataset shape {}'.format(Counter(Y)) )\n",
    "print('Resampled dataset shape {}'.format(Counter(Y_res)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#원래대로 돌리기\n",
    "X=X_res.reshape(1, -1)\n",
    "print( '총 데이터수 : ',X[0].size )\n",
    "print(  '복사된 수 : ',X[0].size - np.unique(X[0]).size )\n",
    "\n",
    "X=X[0].tolist()\n",
    "Y=Y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15698c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0:4])\n",
    "print(Y[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0880b5c",
   "metadata": {},
   "source": [
    "## 3. stratified k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. train, test 나누기\n",
    "#stratified kfold\n",
    "import os\n",
    "import random #데이터 shuffle 사용\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "skf_iris = StratifiedKFold(n_splits=5,shuffle=True,random_state=456)\n",
    "cnt_iter = 0\n",
    "\n",
    "X_train_list = [] #데이터 셋 보관\n",
    "Y_train_list = []\n",
    "\n",
    "X_valid_list = []\n",
    "Y_valid_list = []\n",
    "\n",
    "for train_idx, test_idx in skf_iris.split(X,Y):\n",
    "    \n",
    "    #split으로 반환된 인덱스를 이용하여, 학습 검증용 테스트 데이터 추출\n",
    "    cnt_iter += 1\n",
    "    X_train, X_valid = [X[idx] for idx in train_idx.tolist() ], [X[idx] for idx in test_idx.tolist() ]\n",
    "    Y_train, Y_valid = [Y[idx] for idx in train_idx.tolist() ], [Y[idx] for idx in test_idx.tolist() ]\n",
    "    \n",
    "    X_train_list.append(X_train)\n",
    "    X_valid_list.append(X_valid)\n",
    "    \n",
    "    Y_train_list.append(Y_train)\n",
    "    Y_valid_list.append(Y_valid)\n",
    "    \n",
    "    \n",
    "    #학습 및 예측\n",
    "    \n",
    "    label_train = Y_train\n",
    "    label_test = Y_valid\n",
    "    unique_train, train_counts = np.unique(label_train, return_counts = True)\n",
    "    unique_test, test_counts = np.unique(label_test, return_counts = True)\n",
    "    \n",
    "    uniq_cnt_train = dict(zip(unique_train, train_counts))\n",
    "    uniq_cnt_test = dict(zip(unique_test, test_counts))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('교차 검증 : {}'.format(cnt_iter))\n",
    "    print('학습 레이블 데이터 분포 : \\n', uniq_cnt_train)\n",
    "    print('검증 레이블 데이터 분포 : \\n', uniq_cnt_test,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a663f0",
   "metadata": {},
   "source": [
    "# 데이터 정의\n",
    "- 추가적으로 데이터의 크기를 맞춰주기 위해 3초로 padding 및 truncate 실시 https://sequencedata.tistory.com/25 FixAudioLength\n",
    "- 논문에서는 400frame으로 설정.(여기서는 500frame)\n",
    "- 전처리 방법 결정.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f14052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=50000\n",
    "win_length =  np.int64(50000/40) # 1250\n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "\n",
    "class svd_dataset(Dataset):\n",
    "    def __init__(self,data_path_list,classes,data_num,training,transform=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list[data_num]\n",
    "        self.data_num = data_num\n",
    "        self.training = training\n",
    "        self.label = svd_dataset.get_label(self.path_list,training,data_num)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list,training,data_num):\n",
    "        label_list=[]\n",
    "        \n",
    "        if training:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_train_list[data_num][idx])\n",
    "        else:\n",
    "            for idx,x in enumerate(data_path_list):\n",
    "                label_list.append(Y_valid_list[data_num][idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다.     \n",
    "    \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc를 추출\n",
    "        2. mfcc를 224프레임으로 패딩.\n",
    "        3. resnet에 사용되기 위해 3채널로 복사(rgb 처럼)\n",
    "        4. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig, sr = librosa.load(self.path_list[idx], sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        #stft 500 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "        mel_feature = librosa.feature.melspectrogram(sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature=torch.stack([mel_feature,mel_feature,mel_feature])# 3채널로 복사.\n",
    "            mel_feature = mel_feature.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            mel_feature = torch.from_numpy(mel_feature).type(torch.float32)\n",
    "            mel_feature=mel_feature.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "            \n",
    "        return mel_feature, self.classes.index(self.label[idx]),self.path_list[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=50000\n",
    "win_length =  np.int64(50000/40) # 1250\n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "# test set 제작을 위한 class\n",
    "class svd_test_set(Dataset):\n",
    "    def __init__(self,data_path_list,classes,transform=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list\n",
    "        self.label = svd_test_set.get_label(self.path_list)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list):\n",
    "        label_list=[]\n",
    "        \n",
    "        for idx,x in enumerate(data_path_list):\n",
    "            label_list.append(Y_test[idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다. \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc를 추출\n",
    "        2. mfcc를 224프레임으로 패딩.\n",
    "        3. resnet에 사용되기 위해 3채널로 복사(rgb 처럼)\n",
    "        4. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        sig, sr = librosa.load(self.path_list[idx], sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        \n",
    "        #stft 500 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "        mel_feature = librosa.feature.melspectrogram(sig,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature=torch.stack([mel_feature,mel_feature,mel_feature])# 3채널로 복사.\n",
    "            mel_feature = mel_feature.squeeze(dim=1)\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            mel_feature = torch.from_numpy(mel_feature).type(torch.float32)\n",
    "            mel_feature=mel_feature.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "            \n",
    "        #결과로 마지막에는 path도 출력하게 수정.\n",
    "        return mel_feature, self.classes.index(self.label[idx]), self.path_list[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05129d",
   "metadata": {},
   "source": [
    "# 데이터 로더\n",
    "\n",
    "- grad-cam 사용시 train은 완료했으니, valid test만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89052fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 하이퍼 파라미터\n",
    "BATCH_SIZE =  1 #한 배치당 1개. 왜냐하면 test시 한개만 볼것이라서.\n",
    "EPOCHS = 40 # 전체 데이터 셋을 50번 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                           svd_dataset(\n",
    "                                               X_valid_list,\n",
    "                                               classes,\n",
    "                                               transform = transforms.ToTensor(),\n",
    "                                               data_num=1,\n",
    "                                               training=False\n",
    "                                           ),\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9761d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로더.\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_test_set(\n",
    "                                                   X_test,\n",
    "                                                   classes,\n",
    "                                                   transform = transforms.ToTensor(),\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b15a86",
   "metadata": {},
   "source": [
    "# 데이터 확인\n",
    "\n",
    "grad-map은  valid, test set에서만 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid set 확인\n",
    "#1개 배치 전체 다 가져오는 것.\n",
    "for (valid_data,valid_label,path) in validation_loader:\n",
    "    print(\"X_valid : \",valid_data.size(),'type:',valid_data.type())\n",
    "    print(\"Y_valid : \",valid_label.size(),'type:',valid_label.type())\n",
    "    print(\"path : \",path)\n",
    "    break\n",
    "\n",
    "\n",
    "librosa.display.specshow(valid_data[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "#batch: 32 / 3채널 / frame수: 500  /  feature수: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a38987",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test set 확인\n",
    "#1개 배치 전체 다 가져오는 것.\n",
    "for (test_data,test_label,path) in test_loader:\n",
    "    print(\"X_test : \",test_data.size(),'type:',test_data.type())\n",
    "    print(\"Y_test : \",test_label.size(),'type:',test_label.type())\n",
    "    print(\"path : \",path)\n",
    "    break\n",
    "\n",
    "\n",
    "librosa.display.specshow(test_data[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "#batch: 32 / 3채널 / frame수: 500  /  feature수: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ec67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 확인\n",
    "pd.DataFrame(test_data[0][0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a3143",
   "metadata": {},
   "source": [
    "# grad cam\n",
    "- https://www.secmem.org/blog/2020/01/17/gradcam/\n",
    "- https://blog.naver.com/PostView.nhn?blogId=domodal&logNo=221485292052&parentCategoryNo=&categoryNo=10&viewDate=&isShowPopularPosts=true&from=search\n",
    "- f1-score가 가장높은 5번 모델 cam 확인\n",
    "- 마지막 feature map 부터 output 까지의 gradient를 계산해야한다.\n",
    "- https://sanghyu.tistory.com/46 normalize\n",
    "- https://github.com/PeterKim1/paper_code_review/blob/master/9.%20Visual%20Explanations%20from%20Deep%20Networks%20via%20Gradient-based%20Localization(Grad-CAM)/Grad-CAM_pretrained.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final conv layer name \n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# activations\n",
    "feature_blobs = []\n",
    "\n",
    "# gradient를 가져올 hook 함수\n",
    "backward_feature = []\n",
    "\n",
    "transform_norm = transforms.ToTensor()\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy()) # 레이어의 마지막 output(피처맵)을 구하는 함수\n",
    "    \n",
    "\n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])  #backward시에 그래디언트 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc80d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 수정해서 hook 추가 사용.\n",
    "\n",
    "\n",
    "\n",
    "def model_initialize():\n",
    "    model = models.resnet18(pretrained=True).cuda()\n",
    "    model.ftrs = model.fc.in_features # in_features : fully connected의 입력수.\n",
    "    num_ftrs = model.fc.in_features\n",
    "    \n",
    "    model.layer4.register_forward_hook(hook_feature)\n",
    "    model.layer4.register_backward_hook(backward_hook)\n",
    "    \n",
    "    model.fc = nn.Sequential(nn.Linear(num_ftrs, 256),\n",
    "                             nn.BatchNorm1d(256),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(256,128),\n",
    "                             nn.BatchNorm1d(128),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(128,64),\n",
    "                             nn.BatchNorm1d(64),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(64,50),\n",
    "                             nn.BatchNorm1d(50),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=0.5),\n",
    "                             nn.Linear(50,2)\n",
    "                            )\n",
    "\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "model=model_initialize()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "check_path = '../../checkpoint/checkpoint_melspectro_resnet18_true_ros_2_300.pt'\n",
    "model.load_state_dict(torch.load(check_path))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d66cff",
   "metadata": {},
   "source": [
    "마지막 cnn layer는 layer4로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98c926",
   "metadata": {},
   "source": [
    "## validation set 확인\n",
    "\n",
    "- validation의 cam을 반영하여, test set의 성능을 높히는 것이 목표."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a51d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "\n",
    "\n",
    "#pathology 음성 파일 가져오기\n",
    "\n",
    "length=300\n",
    "image_list=[]\n",
    "\n",
    "label_list=[]\n",
    "prob_list=[] #valid set의 확률값\n",
    "name_list=[] #파일명\n",
    "predict_label_list=[] #예측 라벨\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "mel_list=[]#정규 mel-spectrogram 모은 리스트\n",
    "fallout_list=[] # cam - mel . 음수는 0 처리.\n",
    "grad_list=[]#grad-cam을 담는 배열. amp sum 확인용\n",
    "\n",
    "\n",
    "model.eval()\n",
    "mel_sample=pathology[0]\n",
    "for image, label, path in validation_loader:\n",
    "    \n",
    "    name = path[0].split('\\\\')[-1]\n",
    "    name = name.replace('.wav', '')\n",
    "    name_list.append(name)\n",
    "    label_list.append(classes[label])\n",
    "    \n",
    "    # activations\n",
    "    feature_blobs = []\n",
    "    # gradient를 가져올 hook 함수\n",
    "    backward_feature = []\n",
    "    \n",
    "    mel_sample = image\n",
    "    mel_sample = mel_sample.to(DEVICE).float()\n",
    "\n",
    "\n",
    "    model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "    model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "\n",
    "    # Prediction\n",
    "    logit = model(mel_sample) # 예측값 구하기.\n",
    "    out = F.softmax(logit, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함.)\n",
    "    probs, idx = out.sort(0, True)\n",
    "    print(\"Predicted label : %d, Probability : %.2f\" % (idx[0].item(), probs[0].item()))\n",
    "    predict_label_list.append(classes[idx[0].item()])\n",
    "    prob_list.append(probs[0].item())\n",
    "    res.append(classes[label]==classes[idx[0].item()])\n",
    "    \n",
    "\n",
    "    ###########\n",
    "    # Grad - cam\n",
    "    ###########\n",
    "\n",
    "    score = logit[:, idx[0]].squeeze() # 예측값 y^c.\n",
    "    score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "    \n",
    "    activations = torch.Tensor(feature_blobs[0]).to(DEVICE) # (1, 512, 7, 7), forward activations. append라서 0번이 마지막\n",
    "    gradients = backward_feature[0] # (1, 512, 4, 10), backward gradients. 마지막 conv layer의 gradient\n",
    "    b, k, u, v = gradients.size()  # batch, 피처맵 수,  상, 하\n",
    "    #print(gradients.size())\n",
    "\n",
    "    alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "    weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "\n",
    "    grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "    grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "    grad_cam_map = F.interpolate(grad_cam_map, size=(128, 300), mode='bilinear', align_corners=False) # (1, 1, 128, 300)\n",
    "    map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "    grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 128, 300), min-max scaling\n",
    "\n",
    "    grad_heatmap=grad_cam_map.squeeze().cpu()\n",
    "    grad_list.append(grad_heatmap)\n",
    "    \n",
    "    # grad_cam_map.squeeze() : (128, 300)\n",
    "\n",
    "    #save_image(grad_heatmap, os.path.join(\"./\", \"Grad_CAM.jpg\"))\n",
    "    \n",
    "    # mel-spectorgram min-max normalization\n",
    "    \n",
    "    mel_min,mel_max = mel_sample.squeeze()[0].min(),mel_sample.squeeze()[0].max()\n",
    "    mel_sample = (mel_sample.squeeze()[0]- mel_min).div(mel_max-mel_min)\n",
    "    mel_sample = mel_sample.cpu().numpy()\n",
    "    mel_list.append(mel_sample)\n",
    "    \n",
    "    \n",
    "    #mel_sample = librosa.util.normalize(mel_sample.cpu().squeeze().numpy()[0]) #수정 필요. min-max normalization\n",
    "\n",
    "    grad_result = grad_heatmap.numpy() + mel_sample # (1, 3, 244, 244)\n",
    "    \n",
    "    fallout_list.append(grad_heatmap.numpy() - mel_sample)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    librosa.display.specshow(mel_sample, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    librosa.display.specshow(grad_heatmap.numpy(), sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "    plt.subplot(133)\n",
    "    librosa.display.specshow(grad_result, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if probs[0].item()<0.8:\n",
    "        folder_path='./valid_images/cam_images_0.8/'\n",
    "    elif probs[0].item()<0.9:\n",
    "        folder_path='./valid_images/cam_images_0.9/'\n",
    "    else:\n",
    "        folder_path='./valid_images/cam_images_1.0/'\n",
    "    \n",
    "    plt.savefig(folder_path+name)\n",
    "    plt.close()\n",
    "\n",
    "result_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(predict_label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "result_excel.columns=['name','class','predict','probability','result']\n",
    "result_excel.to_excel(\"./result_excel_valid.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9ebce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#낙폭 feature의 max값 계산.\n",
    "#1d array 계산 먼저.\n",
    "#1d array를 구했으면, 최대값 또는 평균값을 가져오면 끝.\n",
    "\n",
    "fallout_1d = []\n",
    "for idx,fallout in enumerate(fallout_list):\n",
    "    \n",
    "    if prob_list[idx]<0.8:\n",
    "        fallout_folder_path='./valid_images/fallout/cam_images_0.8/'\n",
    "    elif prob_list[idx]<0.9:\n",
    "        fallout_folder_path='./valid_images/fallout/cam_images_0.9/'\n",
    "    else:\n",
    "        fallout_folder_path='./valid_images/fallout/cam_images_1.0/'\n",
    "    \n",
    "    fallout_temp=fallout.max(axis=0) #projection\n",
    "    fallout_temp=np.where(fallout_temp < 0 , 0, fallout_temp)\n",
    "    fallout_1d.append(fallout_temp)\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    librosa.display.specshow(fallout, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"fallout\")\n",
    "    \n",
    "    plt.savefig(fallout_folder_path+name_list[idx])\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude sum과 probabilty의 상관성.\n",
    "grad_ampsum = []\n",
    "grad_area = []\n",
    "\n",
    "for idx,gradcam in enumerate(grad_list):\n",
    "    \n",
    "    ampsum_temp=gradcam.sum().numpy() #amp_sum을 구하는방법\n",
    "    gradcam=np.where(gradcam.numpy()>0,1,0)\n",
    "    gradarea_temp = gradcam.sum()\n",
    "    \n",
    "    \n",
    "    #print(fallout_temp)\n",
    "    grad_ampsum.append(ampsum_temp)\n",
    "    grad_area.append(gradarea_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad의 frequency 방향 projection\n",
    "grad_frequency = []\n",
    "\n",
    "for idx,gradcam in enumerate(grad_list):\n",
    "    \n",
    "    gradcam_temp=gradcam.numpy().max(axis=1) #projection\n",
    "    gradcam_temp=np.mean(gradcam_temp)\n",
    "    grad_frequency.append(gradcam_temp)\n",
    "    \n",
    "grad_frequency    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallout frequency 방향 projection과 probabilty의 상관성.\n",
    "#1d array 계산 먼저.\n",
    "#1d array를 구했으면, 최대값 또는 평균값을 가져오면 끝.\n",
    "\n",
    "fallout_frequency = []\n",
    "for idx,fallout in enumerate(fallout_list):\n",
    "    \n",
    "    fallout_temp=fallout.max(axis=1) #projection\n",
    "    fallout_temp=np.where(fallout_temp < 0 , 0, fallout_temp)\n",
    "    fallout_temp=np.mean(fallout_temp)\n",
    "    fallout_frequency.append(fallout_temp)\n",
    "\n",
    "    \n",
    "fallout_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b02b4",
   "metadata": {},
   "source": [
    "코드 수정해서 확률, 이름 데이터도 같이 저장.\n",
    "\n",
    "그리고 일단 최대값 프로젝션으로 계속해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84537642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fallout1d 저장.\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "with open('./valid_images/fallout1d/fallout.pickle', 'wb') as f:\n",
    "    pickle.dump(fallout_1d, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./valid_images/fallout1d/name.pickle', 'wb') as f:\n",
    "    pickle.dump(name_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./valid_images/fallout1d/prob.pickle', 'wb') as f:\n",
    "    pickle.dump(prob_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('./valid_images/fallout1d/label.pickle', 'wb') as f:\n",
    "    pickle.dump(label_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./valid_images/fallout1d/res.pickle', 'wb') as f:\n",
    "    pickle.dump(res, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./valid_images/fallout1d/fallout.pickle', 'rb') as f:\n",
    "    fallout_1d = pickle.load(f)\n",
    "\n",
    "with open('./valid_images/fallout1d/name.pickle', 'rb') as f:\n",
    "    name_list= pickle.load(f)\n",
    "\n",
    "with open('./valid_images/fallout1d/prob.pickle', 'rb') as f:\n",
    "    prob_list = pickle.load(f)\n",
    "\n",
    "with open('./valid_images/fallout1d/label.pickle', 'rb') as f:\n",
    "    label_list = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('./valid_images/fallout1d/res.pickle', 'rb') as f:\n",
    "    res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd172b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fallout_mean = []    \n",
    "\n",
    "for idx,fallout in enumerate(fallout_1d):\n",
    "    print(name_list[idx],np.mean(fallout),prob_list[idx])\n",
    "    fallout_mean.append(np.mean(fallout))\n",
    "    \n",
    "    if prob_list[idx]<0.8:\n",
    "        fallout_folder_path='./valid_images/fallout1d/cam_images_0.8/'\n",
    "    elif prob_list[idx]<0.9:\n",
    "        fallout_folder_path='./valid_images/fallout1d/cam_images_0.9/'\n",
    "    else:\n",
    "        fallout_folder_path='./valid_images/fallout1d/cam_images_1.0/'\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fallout)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"fallout\")\n",
    "    \n",
    "    plt.savefig(fallout_folder_path+name_list[idx])\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96163ac8",
   "metadata": {},
   "source": [
    "fallout_excel\n",
    "요약 통계량 구하고,\n",
    "산점도 나타내기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(fallout_mean),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "fallout_excel.columns=['name','class','probability','fallout','res']\n",
    "fallout_excel.to_excel(\"./fallout_excel_valid.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel=pd.read_excel(\"./fallout_excel_valid.xlsx\")\n",
    "fallout_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6441488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 형태 변환\n",
    "# 모두 pathology일 확률로 변환\n",
    "\n",
    "\n",
    "# 1. healthy 맞춘 경우. \n",
    "filter1=fallout_excel['res']==True\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_excel['probability']=np.where(filter1&filter2,fallout_excel['probability'],1-fallout_excel['probability'])\n",
    "\n",
    "\n",
    "#2. pathology 틀린 경우\n",
    "filter1=fallout_excel['res']==False\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "fallout_excel['probability']=np.where(filter1&filter2,fallout_excel['probability'],1-fallout_excel['probability'])\n",
    "\n",
    "\n",
    "#확인\n",
    "filter1=fallout_excel['res']==True\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_excel[filter1&filter2]['probability']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.plot(x='class',y='fallout', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edaa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel['res']=fallout_excel['res'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel['fallout_frequency'] = pd.DataFrame(fallout_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f39fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel['grad_frequency'] = pd.DataFrame(grad_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amplitude sum, area 과 probability 상관성 확인을 위한 dataframe\n",
    "ampsum_prob=pd.concat([ pd.DataFrame(grad_ampsum),\n",
    "                       pd.DataFrame(grad_area),\n",
    "                       fallout_excel['probability'],\n",
    "                       fallout_excel['class'],\n",
    "                       fallout_excel['fallout'],\n",
    "                       pd.DataFrame(fallout_frequency)\n",
    "                       ],axis=1)\n",
    "ampsum_prob.columns=['amp_sum','grad_area','probability','class','fallout','fallout_frequency']\n",
    "ampsum_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amplitude sum과 probability 상관성 확인\n",
    "ampsum_prob[['amp_sum','probability']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_area과 probability 상관성 확인\n",
    "ampsum_prob[['grad_area','probability']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_area과 fallout 상관성 확인\n",
    "ampsum_prob[['grad_area','fallout']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e606b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = {'pathology':'red', 'healthy':'green'}\n",
    "ampsum_prob.plot(x='probability',y='amp_sum', kind = 'scatter',c=ampsum_prob['class'].map(colors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e15fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amplitude sum을 변형한 넓이와 probs의 관계\n",
    "colors = {'pathology':'red', 'healthy':'green'}\n",
    "ampsum_prob.plot(x='probability',y='grad_area', kind = 'scatter',c=ampsum_prob['class'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50691c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amplitude sum을 변형한 넓이와 fallout 관계\n",
    "colors = {'pathology':'red', 'healthy':'green'}\n",
    "ampsum_prob.plot(x='fallout',y='grad_area', kind = 'scatter',c=ampsum_prob['class'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amplitude sum을 변형한 넓이와 fallout 관계\n",
    "colors = {'pathology':'red', 'healthy':'green'}\n",
    "ampsum_prob.plot(x='fallout',y='fallout_frequency', kind = 'scatter',c=ampsum_prob['class'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['res']==0\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "fallout_filter1\n",
    "#현재 validation set에서는 8건 밖에 healthy가 틀린경우가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8def99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 데이터.\n",
    "#틀렸을때 어디 위치하는지.\n",
    "#기준은 pathology의 probability\n",
    "colors = {0:'red', 1:'green'}\n",
    "fallout_excel.plot(x='probability',y='fallout', kind = 'scatter',c=fallout_excel['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fallout_freqency\n",
    "colors = {0:'red', 1:'green'}\n",
    "fallout_excel.plot(x='probability',y='fallout_frequency', kind = 'scatter',c=fallout_excel['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4509ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradcam_freqency\n",
    "colors = {0:'red', 1:'green'}\n",
    "fallout_excel.plot(x='probability',y='grad_frequency', kind = 'scatter',c=fallout_excel['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.plot(x='res',y='fallout', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40732778",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.plot(x='res',y='fallout_frequency', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29df26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['probability']>0.9\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "fallout_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d46ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['probability']<0.5\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "filter3=fallout_excel['probability']>0.3\n",
    "fallout_filter2=fallout_excel[filter1&filter2&filter3]\n",
    "fallout_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00240abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathology인 데이터에서, pathology일 확률로 예측 한경우. \n",
    "#probability가 0.5 이하면 그냥 예측에 실패했다.\n",
    "df1=pd.concat([fallout_filter1,fallout_filter2])\n",
    "df1.plot(x='probability',y='fallout', kind = 'scatter',c=df1['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy가 맞는데, pathology라고 0.1 정도 된다고 생각한 경우 \n",
    "filter1=fallout_excel['probability']<0.1\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "fallout_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14698136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy가 맞는데, pathology라고 0.5~0.7라고 생각한 경우 \n",
    "filter1=fallout_excel['probability']>0.5\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "#filter3=fallout_excel['probability']<0.8\n",
    "fallout_filter2=fallout_excel[filter1&filter2]\n",
    "fallout_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy인데 pathology일 확률로 나타낸 그림\n",
    "#예측이 틀린경우는 5건 밖에 없다.\n",
    "#probability가 0.5 이하면 그냥 예측에 실패했다.\n",
    "#healthy인데 pathology가 probabilty가 0.6 이상이라 하면, 그냥 틀려버린다.\n",
    "df2=pd.concat([fallout_filter1,fallout_filter2])\n",
    "df2.plot(x='probability',y='fallout', kind = 'scatter',c=df2['res'].map(colors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,fallout in enumerate(fallout_1d):\n",
    "    if prob_list[idx]>0.9:\n",
    "        print(name_list[idx],np.mean(fallout),prob_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baabb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,fallout in enumerate(fallout_1d):\n",
    "    if prob_list[idx]<0.8:\n",
    "        print(name_list[idx],np.mean(fallout),prob_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,fallout in enumerate(fallout_1d):\n",
    "    print(name_list[idx],np.max(fallout),prob_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c465ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./valid_images/fallout1d/fallout.pickle', 'rb') as f:\n",
    "    fallout_1d = pickle.load(f)\n",
    "fallout_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_list[0])\n",
    "plt.plot(fallout_1d[0])\n",
    "print(np.mean(fallout_1d[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e1546",
   "metadata": {},
   "source": [
    "## test set 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdec93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "\n",
    "\n",
    "#pathology 음성 파일 가져오기\n",
    "\n",
    "length=300\n",
    "image_list=[]\n",
    "\n",
    "label_list=[]\n",
    "prob_list=[] #valid set의 확률값\n",
    "name_list=[] #파일명\n",
    "predict_label_list=[] #예측 라벨\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "mel_list=[]#정규 mel-spectrogram 모은 리스트\n",
    "fallout_list=[] # cam - mel . 음수는 0 처리.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "mel_sample=pathology[0]\n",
    "for image, label, path in test_loader:\n",
    "    \n",
    "    name = path[0].split('\\\\')[-1]\n",
    "    name = name.replace('.wav', '')\n",
    "    name_list.append(name)\n",
    "    label_list.append(classes[label])\n",
    "    \n",
    "    # activations\n",
    "    feature_blobs = []\n",
    "    # gradient를 가져올 hook 함수\n",
    "    backward_feature = []\n",
    "    \n",
    "    mel_sample = image\n",
    "    mel_sample = mel_sample.to(DEVICE).float()\n",
    "\n",
    "\n",
    "    model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "    model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "    \n",
    "\n",
    "    # Prediction\n",
    "    logit = model(mel_sample) # 예측값 구하기.\n",
    "    out = F.softmax(logit, dim=1).data.squeeze() # softmax 적용 (모델을 통과는 했지만, criterion는 안통과함.)\n",
    "    probs, idx = out.sort(0, True)\n",
    "    print(\"Predicted label : %d, Probability : %.2f\" % (idx[0].item(), probs[0].item()))\n",
    "    predict_label_list.append(classes[idx[0].item()])\n",
    "    prob_list.append(probs[0].item())\n",
    "    res.append(classes[label]==classes[idx[0].item()])\n",
    "    \n",
    "\n",
    "    ###########\n",
    "    # Grad - cam\n",
    "    ###########\n",
    "\n",
    "    score = logit[:, idx[0]].squeeze() # 예측값 y^c.\n",
    "    score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "    \n",
    "    activations = torch.Tensor(feature_blobs[0]).to(DEVICE) # (1, 512, 7, 7), forward activations. append라서 0번이 마지막\n",
    "    gradients = backward_feature[0] # (1, 512, 4, 10), backward gradients. 마지막 conv layer의 gradient\n",
    "    b, k, u, v = gradients.size()  # batch, 피처맵 수,  상, 하\n",
    "    #print(gradients.size())\n",
    "\n",
    "    alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "    weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "\n",
    "    grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "    grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "    grad_cam_map = F.interpolate(grad_cam_map, size=(128, 300), mode='bilinear', align_corners=False) # (1, 1, 128, 300)\n",
    "    map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "    grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 128, 300), min-max scaling\n",
    "\n",
    "    grad_heatmap=grad_cam_map.squeeze().cpu()\n",
    "    \n",
    "    # grad_cam_map.squeeze() : (128, 300)\n",
    "\n",
    "    #save_image(grad_heatmap, os.path.join(\"./\", \"Grad_CAM.jpg\"))\n",
    "    \n",
    "    # mel-spectorgram min-max normalization\n",
    "    \n",
    "    mel_min,mel_max = mel_sample.squeeze()[0].min(),mel_sample.squeeze()[0].max()\n",
    "    mel_sample = (mel_sample.squeeze()[0]- mel_min).div(mel_max-mel_min)\n",
    "    mel_sample = mel_sample.cpu().numpy()\n",
    "    mel_list.append(mel_sample)\n",
    "    \n",
    "    \n",
    "    #mel_sample = librosa.util.normalize(mel_sample.cpu().squeeze().numpy()[0]) #수정 필요. min-max normalization\n",
    "\n",
    "    grad_result = grad_heatmap.numpy() + mel_sample # (1, 3, 244, 244)\n",
    "    \n",
    "    fallout_list.append(grad_heatmap.numpy() - mel_sample)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    librosa.display.specshow(mel_sample, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    librosa.display.specshow(grad_heatmap.numpy(), sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "\n",
    "    plt.subplot(133)\n",
    "    librosa.display.specshow(grad_result, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Spectrogram (dB)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if probs[0].item()<0.8:\n",
    "        folder_path='./cam_images_0.8/'\n",
    "    elif probs[0].item()<0.9:\n",
    "        folder_path='./cam_images_0.9/'\n",
    "    else:\n",
    "        folder_path='./cam_images_1.0/'\n",
    "    \n",
    "    plt.savefig(folder_path+name)\n",
    "    \n",
    "\n",
    "result_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(predict_label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "result_excel.columns=['name','class','predict','probability','result']\n",
    "result_excel.to_excel(\"./result_excel_test.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056d830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#낙폭 feature의 max값 계산.\n",
    "#1d array 계산 먼저.\n",
    "#1d array를 구했으면, 최대값 또는 평균값을 가져오면 끝.\n",
    "\n",
    "fallout_1d = []\n",
    "for idx,fallout in enumerate(fallout_list):\n",
    "    \n",
    "    if prob_list[idx]<0.8:\n",
    "        fallout_folder_path='./fallout/cam_images_0.8/'\n",
    "    elif prob_list[idx]<0.9:\n",
    "        fallout_folder_path='./fallout/cam_images_0.9/'\n",
    "    else:\n",
    "        fallout_folder_path='./fallout/cam_images_1.0/'\n",
    "    \n",
    "    fallout_temp=fallout.max(axis=0) #projection\n",
    "    fallout_temp=np.where(fallout_temp < 0 , 0, fallout_temp)\n",
    "    fallout_1d.append(fallout_temp)\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    librosa.display.specshow(fallout, sr=sr, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"fallout\")\n",
    "    \n",
    "    plt.savefig(fallout_folder_path+name_list[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657df3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fallout1d 저장.\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "with open('./fallout1d/fallout.pickle', 'wb') as f:\n",
    "    pickle.dump(fallout_1d, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./fallout1d/name.pickle', 'wb') as f:\n",
    "    pickle.dump(name_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./fallout1d/prob.pickle', 'wb') as f:\n",
    "    pickle.dump(prob_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('./fallout1d/label.pickle', 'wb') as f:\n",
    "    pickle.dump(label_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./fallout1d/res.pickle', 'wb') as f:\n",
    "    pickle.dump(res, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97001dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./fallout1d/fallout.pickle', 'rb') as f:\n",
    "    fallout_1d = pickle.load(f)\n",
    "\n",
    "with open('./fallout1d/name.pickle', 'rb') as f:\n",
    "    name_list= pickle.load(f)\n",
    "\n",
    "with open('./fallout1d/prob.pickle', 'rb') as f:\n",
    "    prob_list = pickle.load(f)\n",
    "\n",
    "with open('./fallout1d/label.pickle', 'rb') as f:\n",
    "    label_list = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('./fallout1d/res.pickle', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "\n",
    "fallout_mean = []    \n",
    "\n",
    "for idx,fallout in enumerate(fallout_1d):\n",
    "    print(name_list[idx],np.mean(fallout),prob_list[idx])\n",
    "    fallout_mean.append(np.mean(fallout))\n",
    "    \n",
    "    if prob_list[idx]<0.8:\n",
    "        fallout_folder_path='./fallout1d/cam_images_0.8/'\n",
    "    elif prob_list[idx]<0.9:\n",
    "        fallout_folder_path='./fallout1d/cam_images_0.9/'\n",
    "    else:\n",
    "        fallout_folder_path='./fallout1d/cam_images_1.0/'\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fallout)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"fallout\")\n",
    "    \n",
    "    plt.savefig(fallout_folder_path+name_list[idx])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f89786",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel=pd.concat( [pd.DataFrame(name_list),\n",
    "            pd.DataFrame(label_list),\n",
    "            pd.DataFrame(prob_list),\n",
    "            pd.DataFrame(fallout_mean),\n",
    "            pd.DataFrame(res)],axis=1)\n",
    "fallout_excel.columns=['name','class','probability','fallout','res']\n",
    "fallout_excel.to_excel(\"./fallout_excel_test.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2582938",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel=pd.read_excel(\"./fallout_excel_test.xlsx\")\n",
    "fallout_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105379ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 형태 변환\n",
    "# 모두 pathology일 확률로 변환\n",
    "\n",
    "\n",
    "# 1. healthy 맞춘 경우. \n",
    "filter1=fallout_excel['res']==True\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_excel['probability']=np.where(filter1&filter2,fallout_excel['probability'],1-fallout_excel['probability'])\n",
    "\n",
    "\n",
    "#2. pathology 틀린 경우\n",
    "filter1=fallout_excel['res']==False\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "fallout_excel['probability']=np.where(filter1&filter2,fallout_excel['probability'],1-fallout_excel['probability'])\n",
    "\n",
    "\n",
    "#확인\n",
    "filter1=fallout_excel['res']==True\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_excel[filter1&filter2]['probability']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.plot(x='class',y='fallout', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel['res']=fallout_excel['res'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['res']==0\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "print(fallout_filter1.shape[0])\n",
    "fallout_filter1\n",
    "#현재 test set에서는 42건 healthy가 틀린경우가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['res']==0\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "print(fallout_filter1.shape[0])\n",
    "fallout_filter1\n",
    "#현재 test set에서는 23건 pathology가 틀린경우가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28828d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 데이터.\n",
    "#틀렸을때 어디 위치하는지.\n",
    "#기준은 pathology의 probability\n",
    "colors = {0:'red', 1:'green'}\n",
    "fallout_excel.plot(x='probability',y='fallout', kind = 'scatter',c=fallout_excel['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49862249",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.plot(x='res',y='fallout', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallout_excel.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['probability']>0.9\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "fallout_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9512563",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=fallout_excel['probability']<0.5\n",
    "filter2=fallout_excel['class']=='pathology'\n",
    "filter3=fallout_excel['probability']>0.2\n",
    "fallout_filter2=fallout_excel[filter1&filter2&filter3]\n",
    "fallout_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathology인 데이터에서, pathology일 확률로 예측 한경우. \n",
    "#probability가 0.5 이하면 그냥 예측에 실패했다.\n",
    "df1=pd.concat([fallout_filter1,fallout_filter2])\n",
    "df1.plot(x='probability',y='fallout', kind = 'scatter',c=df1['res'].map(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy가 맞는데, pathology라고 0.1 정도 된다고 생각한 경우 \n",
    "filter1=fallout_excel['probability']<0.1\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "fallout_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd88482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy가 맞는데, pathology라고 0.5~0.7라고 생각한 경우 \n",
    "filter1=fallout_excel['probability']>0.5\n",
    "filter2=fallout_excel['class']=='healthy'\n",
    "#filter3=fallout_excel['probability']<0.8\n",
    "fallout_filter2=fallout_excel[filter1&filter2]\n",
    "fallout_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#healthy인데 pathology일 확률로 나타낸 그림\n",
    "#예측이 틀린경우는 5건 밖에 없다.\n",
    "#probability가 0.5 이하면 그냥 예측에 실패했다.\n",
    "#healthy인데 pathology가 probabilty가 0.6 이상이라 하면, 그냥 틀려버린다.\n",
    "df2=pd.concat([fallout_filter1,fallout_filter2])\n",
    "df2.plot(x='probability',y='fallout', kind = 'scatter',c=df2['res'].map(colors))\n",
    "\n",
    "\n",
    "#healthy는 fallout에 의존하지 않음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87176135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af8524",
   "metadata": {},
   "source": [
    "# 데이터 추출\n",
    "\n",
    "fallout < 0.4 이고, healthy로 예측한 데이터들을 엑셀로 추출한다.\n",
    "\n",
    "그 데이터를 fusion network에 다시 삽입한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. fallout < 0.4 이고, healthy로 예측한 데이터\n",
    "\n",
    "filter1=fallout_excel['probability']<0.5\n",
    "filter2=fallout_excel['fallout']<0.4\n",
    "\n",
    "fallout_filter1=fallout_excel[filter1&filter2]\n",
    "print(fallout_filter1['res'].shape[0],'건 추출.')\n",
    "print(np.sum(fallout_filter1['res']),'건 맞춤.')\n",
    "print(np.sum(fallout_filter1['res']==0),'건 틀림.')\n",
    "\n",
    "\n",
    "fallout_filter1\n",
    "\n",
    "#총 92건의 데이터를 다시 추론 할것이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d54cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 이름 추출.\n",
    "path_phrase = \"../../../voice_data/fusion/\"\n",
    "fallout_fusion_data=fallout_filter1[['name','class']].copy()\n",
    "\n",
    "fallout_fusion_data['name']=path_phrase+fallout_fusion_data['class']+'/phrase\\\\'+fallout_fusion_data['name']+'.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f815fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = fallout_fusion_data['name'].to_list()\n",
    "Y_test = fallout_fusion_data['class'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ab68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edeed1",
   "metadata": {},
   "source": [
    "# 데이터 퓨전 네트워크 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32da64",
   "metadata": {},
   "source": [
    "## 1. p a i u fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1197aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#3. 하이퍼 파라미터\n",
    "BATCH_SIZE =  16 #한 배치당 16개 음성데이터\n",
    "\n",
    "\n",
    "classes = [\"pathology\",\"healthy\"]\n",
    "sr=50000\n",
    "win_length =  np.int64(50000/40) # 1250\n",
    "n_fft= win_length # WINDOWS SIZE중 사용할 길이. WINDOW SIZE가 넘어가면 나머지 것들은 zero padding\n",
    "hop_length= np.int64( np.ceil(win_length/4) ) #  얼마만큼 시간 주기(sample)를 이동하면서 분석을 할 것인지. 일반적으로 window size의 1/4\n",
    "#또는 10ms만큼으로 한다고 한다.\n",
    "#hop_length가 mfcc의 frame수를 결정한다.\n",
    "\n",
    "\n",
    "# test set 제작을 위한 class\n",
    "class svd_test_set(Dataset):\n",
    "    def __init__(self,data_path_list,classes,transform=None):\n",
    "        #클래스에서 사용할 인자를 받아 인스턴스 변수로 저장하는 일을 한다.\n",
    "        #예를들면, 이미지의 경로 리스트를 저장하는 일을 하게 된다.\n",
    "        \n",
    "        #data_num : k 개 데이터 셋 중 어떤것을 쓸지\n",
    "        #test인지 아닌지.\n",
    "        \n",
    "        self.path_list = data_path_list\n",
    "        self.label = svd_test_set.get_label(self.path_list)\n",
    "        self.classes=classes\n",
    "        self.transform=transform\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def get_label(cls,data_path_list):\n",
    "        label_list=[]\n",
    "        \n",
    "        for idx,x in enumerate(data_path_list):\n",
    "            label_list.append(Y_test[idx])\n",
    "        #print(label_list)\n",
    "        return label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        #데이터 셋의 길이를 정수로 반환한다. \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. path를 받아서, 소리에서 mfcc를 추출\n",
    "        2. mfcc를 224프레임으로 패딩.\n",
    "        3. resnet에 사용되기 위해 3채널로 복사(rgb 처럼)\n",
    "        4. 0~1 정규화\n",
    "        \n",
    "        \"\"\"\n",
    "        kind=self.path_list[idx].split('/')[-2]\n",
    "        number=self.path_list[idx].split('\\\\')[-1].split('-')[0]\n",
    "        path_a=\"../../../voice_data/fusion/\"+kind+\"/a\\\\\"+number+\"-a_n.wav\"\n",
    "        path_i=\"../../../voice_data/fusion/\"+kind+\"/i\\\\\"+number+\"-i_n.wav\"\n",
    "        path_u=\"../../../voice_data/fusion/\"+kind+\"/u\\\\\"+number+\"-u_n.wav\"\n",
    "\n",
    "        sig_phrase, sr = librosa.load(self.path_list[idx], sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        sig_a, sr = librosa.load(path_a, sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        sig_i, sr = librosa.load(path_i, sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        sig_u, sr = librosa.load(path_u, sr=50000)# 논문에서 f_s = 50 000HZ\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #stft 500 FRAME이 되도록 패딩.\n",
    "        length = 300\n",
    "        mel_feature = librosa.feature.melspectrogram(sig_phrase,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature = librosa.core.power_to_db(mel_feature,ref=np.max)\n",
    "        \n",
    "        mel_feature_a = librosa.feature.melspectrogram(sig_a,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature_a = librosa.core.power_to_db(mel_feature_a,ref=np.max)\n",
    "        \n",
    "        mel_feature_i = librosa.feature.melspectrogram(sig_i,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature_i = librosa.core.power_to_db(mel_feature_i,ref=np.max)\n",
    "        \n",
    "        mel_feature_u = librosa.feature.melspectrogram(sig_u,sr=sr,hop_length=hop_length,n_fft=n_fft)\n",
    "        mel_feature_u = librosa.core.power_to_db(mel_feature_u,ref=np.max)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #mel_feature=librosa.util.normalize(mel_feature) # l-infinity norm\n",
    "        \n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        mel_feature = pad2d(mel_feature, length)\n",
    "        mel_feature_a = pad2d(mel_feature_a, length)\n",
    "        mel_feature_i = pad2d(mel_feature_i, length)\n",
    "        mel_feature_u = pad2d(mel_feature_u, length)\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            #print('transform')\n",
    "            mel_feature=self.transform(mel_feature).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature=torch.stack([mel_feature,mel_feature,mel_feature])# 3채널로 복사.\n",
    "            mel_feature = mel_feature.squeeze(dim=1)\n",
    "            \n",
    "            mel_feature_a=self.transform(mel_feature_a).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature_a=torch.stack([mel_feature_a,mel_feature_a,mel_feature_a])# 3채널로 복사.\n",
    "            mel_feature_a = mel_feature_a.squeeze(dim=1)            \n",
    "            \n",
    "            mel_feature_i=self.transform(mel_feature_i).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature_i=torch.stack([mel_feature_i,mel_feature_i,mel_feature_i])# 3채널로 복사.\n",
    "            mel_feature_i = mel_feature_i.squeeze(dim=1)       \n",
    "            \n",
    "            mel_feature_u=self.transform(mel_feature_u).type(torch.float32)# 데이터 0~1 정규화\n",
    "            mel_feature_u=torch.stack([mel_feature_u,mel_feature_u,mel_feature_u])# 3채널로 복사.\n",
    "            mel_feature_u = mel_feature_u.squeeze(dim=1)                 \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            mel_feature = torch.from_numpy(mel_feature).type(torch.float32)\n",
    "            mel_feature=mel_feature.unsqueeze(0)#cnn 사용위해서 추가\n",
    "            #MFCCs = MFCCs.permute(2, 0, 1)\n",
    "        return mel_feature,mel_feature_a,mel_feature_i,mel_feature_u,self.classes.index(self.label[idx])\n",
    "    \n",
    "\n",
    "    \n",
    "# 테스트 데이터 로더.\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = \n",
    "                                               svd_test_set(\n",
    "                                                   X_test,\n",
    "                                                   classes,\n",
    "                                                   transform = transforms.ToTensor(),\n",
    "                                               ),\n",
    "                                               batch_size = BATCH_SIZE,\n",
    "                                               shuffle = True,) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (X_test,a_test,i_test,u_test,Y_test) in test_loader:\n",
    "    print(\"X_test : \",X_test.size(),'\\na : ',a_test.size(),'\\ni : ',i_test.size(),'\\nu : ',u_test.size())\n",
    "    print(\"Y_test : \",Y_test.size(),'type:',Y_test.type())\n",
    "    break\n",
    "    \n",
    "\n",
    "\n",
    "print(Y_test[0])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(141)\n",
    "librosa.display.specshow(X_test[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.subplot(142)\n",
    "librosa.display.specshow(a_test[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.subplot(143)\n",
    "librosa.display.specshow(i_test[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.subplot(144)\n",
    "librosa.display.specshow(u_test[0][0].numpy(), sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "#batch: 32 / 3채널 / feature수: 128/ frame수: 300   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c97047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 네트워크\n",
    "\n",
    "class resnet18_feature(nn.Module):\n",
    "    def __init__(self, embedding_dimension=128, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # embedding\n",
    "        input_features_fc_layer = self.model.fc.in_features # fc layer 채널 수 얻기\n",
    "        self.model.fc = nn.Linear(input_features_fc_layer, embedding_dimension, bias=True) # fc layer 수정\n",
    "        \n",
    "    def forward(self, images):\n",
    "        embedding = self.model(images) # embedding 생성\n",
    "        return embedding\n",
    "\n",
    "    \n",
    "class resnet18_fusion(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class=2):\n",
    "        super(resnet18_fusion, self).__init__()\n",
    "\n",
    "\n",
    "        model1 = resnet18_feature()\n",
    "        self.phrase_model = model1\n",
    "        in_features = self.phrase_model.model.fc.out_features\n",
    "        \n",
    "\n",
    "        model2 = resnet18_feature()\n",
    "        self.a_model = model2\n",
    "\n",
    "        model3 = resnet18_feature()\n",
    "        self.i_model = model3\n",
    "\n",
    "        model4 = resnet18_feature()\n",
    "        self.u_model = model4\n",
    "        \n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(in_features * 4, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(64,50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(50,n_class)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, phrase,a,i,u):\n",
    "        phrase_feature = self.phrase_model(phrase)\n",
    "        a_feature = self.a_model(a)\n",
    "        i_feature = self.i_model(i)\n",
    "        u_feature = self.u_model(u)      \n",
    "        \n",
    "        #0204 여기부터 수정.\n",
    "\n",
    "        combine = torch.cat((phrase_feature,\n",
    "                             a_feature,\n",
    "                             i_feature,\n",
    "                             u_feature), 1)\n",
    "        combine = self.combine(combine)\n",
    "\n",
    "        return combine\n",
    "    \n",
    "    \n",
    "# 모델 \n",
    "# pretrained\n",
    "\n",
    "\n",
    "def model_initialize():\n",
    "    model = resnet18_fusion(n_class=2).cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "model= model_initialize()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix 계산\n",
    "#test set 계산.\n",
    "def test_evaluate(model,test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    #no_grad : 그래디언트 값 계산 막기.\n",
    "    with torch.no_grad():\n",
    "        for phrase,a,i,u,label in test_loader:\n",
    "            phrase = phrase.to(DEVICE)\n",
    "            a = a.to(DEVICE)\n",
    "            i = i.to(DEVICE)        \n",
    "            u = u.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(phrase,a,i,u)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # 가장 확률이 높은 class 1개를 가져온다.그리고 인덱스만\n",
    "            answers +=label\n",
    "            predictions +=prediction\n",
    "            \n",
    "        return predictions,answers,test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84384bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (resnet18)\n",
    "# kfold의 confusion matrix는 계산 방법이 다르다.\n",
    "# 모델을 각각 불러와서 test set을 평가한다.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cf = np.zeros((2,2))\n",
    "cf_list = []\n",
    "average_accuracy = 0\n",
    "average_fscore = 0\n",
    "\n",
    "for data_ind in range(1,6):\n",
    "\n",
    "    check_path = '../../checkpoint/checkpoint_melspectro_resnet18_true_ros_'+str(data_ind)+'_300_fusion.pt'\n",
    "    model.load_state_dict(torch.load(check_path))\n",
    "\n",
    "    predictions,answers,test_loss = test_evaluate(model, test_loader)\n",
    "    predictions=[ dat.cpu().numpy() for dat in predictions]\n",
    "    answers=[ dat.cpu().numpy() for dat in answers]\n",
    "\n",
    "    \n",
    "    cf = confusion_matrix(answers, predictions)\n",
    "    cf_list.append(cf)\n",
    "    \n",
    "    acc = (cf[0,0]+cf[1,1])/(cf[0,0]+cf[0,1]+cf[1,0]+cf[1,1])\n",
    "    average_accuracy+=acc\n",
    "    precision=cf[0,0]/(cf[0,0]+cf[1,0])\n",
    "    recall=cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "    #fscore=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    #fscroe macro추가\n",
    "    fscore = f1_score(answers,predictions,average='macro')\n",
    "    average_fscore+=fscore\n",
    "    \n",
    "    print('{}번 모델'.format(data_ind))\n",
    "    print(\"Accuracy : {:.4f}% \".format(acc*100))\n",
    "    print(\"Precision (pathology 예측한 것중 맞는 것) : {:.4f}\".format(precision))\n",
    "    print(\"recall (실제 pathology 중  예측이 맞는 것) : {:.4f}\".format(recall))\n",
    "    print(\"f score : {:.4f} \".format(fscore))\n",
    "    print(cf)\n",
    "    print(\"-----\")\n",
    "\n",
    "print(\"평균 acc : {:.4f}\".format(average_accuracy/5))\n",
    "print(\"평균 f1score : {:.4f}\".format(average_fscore/5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961be2b4",
   "metadata": {},
   "source": [
    "3번 모델에서  76개를 맞추어, 5개를 더 맞추게 되었다.\n",
    "\n",
    "결과적으로 82.24 % -> 83.06%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
